# scanners/sheetsinfo_scanner.py

import time
from datetime import datetime, timedelta
from collections import defaultdict

from bot.settings_access import is_scanner_enabled
from core.data import load_sheetsinfo_tasks
from utils.logger import log_to_file, log_separator, log_section
from utils.floor_resolver import get_floor_by_table_name
from core.token_manager import TokenManager
from sqlalchemy.orm import Session

from database.db_models import MistakeStorage, FeedbackStorage, FeedbackStatus, ScheduleOT, TrackedTables
from utils.db_orm import get_max_last_row

from core.config import (
    SHEETSINFO_LOG,
    SHEETINFO_INTERVAL,
    FLOORS
)
from utils.db_orm import (
    update_task_scan_fields,
    update_task_process_fields,
    update_task_update_fields
)
from utils.utils import (
    load_credentials,
    check_sheet_exists,
    batch_get,
    batch_update,
)

class SheetsInfoScanner:
    def __init__(self, session, token_map, doc_id_map):
        self.session = session
        self.token_map = token_map
        self.doc_id_map = doc_id_map
        self.log_file = SHEETSINFO_LOG
        self.tasks = []

    def run(self):
        try:
            manager = TokenManager(self.token_map)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ TokenManager: {e}")
            raise


        while True:
            try:
                if not is_scanner_enabled("sheets_scanner"):
                    # log_to_file(self.log_file, "‚è∏ –°–∫–∞–Ω–µ—Ä –æ—Ç–∫–ª—é—á—ë–Ω (sheets_scanner). –û–∂–∏–¥–∞–Ω–∏–µ...")
                    time.sleep(10)
                    continue

                log_section("‚ñ∂Ô∏è SheetsInfo –ê–∫—Ç–∏–≤–µ–Ω. –ù–æ–≤—ã–π —Ü–∏–∫–ª —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", self.log_file)

                try:

                    self.token_name, token_path = manager.select_best_token(self.log_file)
                    log_to_file(self.log_file, f"üîë –í—ã–±—Ä–∞–Ω {self.token_name}")
                    self.service = load_credentials(token_path, self.log_file, self.session)
                    log_to_file(self.log_file, f"üîê –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–∫–µ–Ω: {self.token_name}")
                except Exception as e:
                    log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ —Ç–æ–∫–µ–Ω–∞: {e}")
                    time.sleep(10)
                    continue

                for phase_name, method in [
                    ("–∑–∞–≥—Ä—É–∑–∫–∏ –∑–∞–¥–∞—á", self.load_tasks),
                    ("—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", self.scan_phase),
                    ("–æ–±—Ä–∞–±–æ—Ç–∫–∏", self.process_phase),
                    ("–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è", self.update_phase)
                ]:
                    try:
                        method()
                    except Exception as e:
                        log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ {phase_name}: {e}")
                        raise

                # log_section(f"üîÑ –¶–∏–∫–ª –∑–∞–≤–µ—Ä—à—ë–Ω. –°–ª–µ–¥—É—é—â–µ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ {SHEETINFO_INTERVAL} —Å–µ–∫—É–Ω–¥", self.log_file)
                # log_to_file(self.log_file, "\n" * 5)

            except Exception as e:
                # log_separator(self.log_file)
                # log_to_file(self.log_file, f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
                time.sleep(10)
                continue

            finally:
                self.session.close()
            time.sleep(SHEETINFO_INTERVAL)

#############################################################################################
# –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–¥–∞—á –∏–∑ –ë–î
#############################################################################################

    def load_tasks(self):
        # log_section("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–¥–∞—á –∏–∑ SheetsInfo", self.log_file)

        self.tasks = load_sheetsinfo_tasks(self.session, self.log_file)

        if not self.tasks:
            # log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.")
            self.tasks = []
            return

        # log_to_file(self.log_file, f"üîÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–¥–∞—á: {len(self.tasks)}")

        skipped = 0
        for task in self.tasks:
            if not task.assign_doc_ids(self.doc_id_map):
                skipped += 1

        # if skipped:
        #     log_to_file(self.log_file, f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –∑–∞–¥–∞—á –±–µ–∑ doc_id: {skipped}")

#############################################################################################
# –§–∞–∑–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
#############################################################################################

    def scan_phase(self):
        # log_section("üîç –§–∞–∑–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", self.log_file)

        if not self.tasks:
            # log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.")
            return

        ready_tasks = [task for task in self.tasks if task.is_ready_to_scan()]
        if not ready_tasks:
            # log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á, –≥–æ—Ç–æ–≤—ã—Ö –∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é.")
            return

        # log_to_file(self.log_file, f"üîé –ù–∞–π–¥–µ–Ω–æ {len(ready_tasks)} –∑–∞–¥–∞—á, –≥–æ—Ç–æ–≤—ã—Ö –∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é:")

        scan_groups = defaultdict(list)
        for task in ready_tasks:
            if not task.assign_doc_ids(self.doc_id_map):
                # log_to_file(self.log_file, f"‚ö†Ô∏è [Task {task.name_of_process}] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å doc_id. –ü—Ä–æ–ø—É—Å–∫.")
                continue
            scan_groups[task.scan_group].append(task)

        for scan_group, group_tasks in scan_groups.items():
            # log_separator(self.log_file)
            # log_to_file(self.log_file, f"üìò –û–±—Ä–∞–±–æ—Ç–∫–∞ scan_group: {scan_group} ({len(group_tasks)} –∑–∞–¥–∞—á)")

            if not group_tasks:
                # log_to_file(self.log_file, "‚ö™ –í –≥—Ä—É–ø–ø–µ –Ω–µ—Ç –∑–∞–¥–∞—á.")
                continue

            doc_id = group_tasks[0].source_doc_id
            unique_sheet_names = set(task.source_page_name for task in group_tasks)
            # log_to_file(self.log_file, f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ª–∏—Å—Ç–æ–≤: {unique_sheet_names}")

            exists_map = {
                sheet_name: check_sheet_exists(self.service, doc_id, sheet_name, self.log_file, self.token_name, self.session)
                for sheet_name in unique_sheet_names
            }

            # for sheet_name, exists in exists_map.items():
            #     log_to_file(self.log_file, f"{'‚úÖ' if exists else '‚ö†Ô∏è'} –õ–∏—Å—Ç '{sheet_name}' {'—Å—É—â–µ—Å—Ç–≤—É–µ—Ç' if exists else '–Ω–µ –Ω–∞–π–¥–µ–Ω'}.")

            valid_tasks = []
            for task in group_tasks:
                sheet_name = task.source_page_name
                if exists_map.get(sheet_name):
                    # log_to_file(self.log_file, f"‚û°Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º '{sheet_name}' –¥–ª—è –∑–∞–¥–∞—á–∏ {task.name_of_process}.")
                    valid_tasks.append(task)
                else:
                    # log_to_file(self.log_file, f"‚õî –ü—Ä–æ–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ {task.name_of_process}: –ª–∏—Å—Ç '{sheet_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω.")
                    task.update_after_scan(success=False)
                    update_task_scan_fields(self.session, task, self.log_file, table_name="SheetsInfo")

            if not valid_tasks:
                # log_to_file(self.log_file, f"‚ö™ –í—Å–µ –∑–∞–¥–∞—á–∏ –≥—Ä—É–ø–ø—ã {scan_group} –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã. –ü—Ä–æ–ø—É—Å–∫ batchGet.")
                continue

            range_to_tasks = defaultdict(list)
            for task in valid_tasks:
                range_str = f"{task.source_page_name}!{task.source_page_area}"
                range_to_tasks[range_str].append(task)

            ranges = list(range_to_tasks.keys())

            # log_to_file(self.log_file, "")
            # log_to_file(self.log_file, f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ batchGet –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç {task.source_table_type} —Å {len(ranges)} —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏:")
            
            # for r in ranges:
            #     log_to_file(self.log_file, f"   ‚Ä¢ {r}")

            response_data = batch_get(
                self.service,
                doc_id,
                ranges,
                scan_group,
                self.log_file,
                self.token_name,
                self.session
            )
            if not response_data:
                # log_to_file(self.log_file, "‚ùå –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç batchGet. –í—Å–µ –∑–∞–¥–∞—á–∏ –±—É–¥—É—Ç –æ—Ç–º–µ—á–µ–Ω—ã –∫–∞–∫ –Ω–µ—É–¥–∞—á–Ω—ã–µ.")
                for task in valid_tasks:
                    task.update_after_scan(success=False)
                    update_task_scan_fields(self.session, task, self.log_file, table_name="SheetsInfo")
                continue

            normalized_response = {}
            for k, v in response_data.items():
                clean_key = k.replace("'", "")
                if "!" in clean_key:
                    sheet_name, cells_range = clean_key.split("!", 1)
                    normalized_response[(sheet_name.strip(), cells_range.strip())] = v

            # log_to_file(self.log_file, "")
            # log_to_file(self.log_file, f"üì• –ü–æ–ª—É—á–µ–Ω—ã –¥–∏–∞–ø–∞–∑–æ–Ω—ã: {list(normalized_response.keys())}")

            for task in valid_tasks:
                expected_sheet = task.source_page_name.strip()
                expected_area_start = task.source_page_area.split(":")[0].strip()
                matched_values = None

                for (sheet_name, cells_range), values in normalized_response.items():
                    if sheet_name == expected_sheet and cells_range.startswith(expected_area_start):
                        matched_values = values
                        break

                if matched_values:
                    task.raw_values_json = matched_values
                    task.update_after_scan(success=True)
                    update_task_scan_fields(self.session, task, self.log_file, table_name="SheetsInfo")
                    # log_to_file(self.log_file, f"‚úÖ [Task {task.name_of_process}] –ù–∞–π–¥–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω {sheet_name}!{cells_range}, —Å—Ç—Ä–æ–∫: {len(matched_values)}")
                else:
                    task.update_after_scan(success=False)
                    update_task_scan_fields(self.session, task, self.log_file, table_name="SheetsInfo")
                    # log_to_file(self.log_file, f"‚ö†Ô∏è [Task {task.name_of_process}] –î–∏–∞–ø–∞–∑–æ–Ω {expected_sheet}!{task.source_page_area} –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç.")

        # for task in self.tasks:
        #     log_to_file(
        #         self.log_file,
        #         f"‚ö™ [Task {task.name_of_process}] –û—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ: {task.scanned} | "
        #         f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {task.proceed} | –ò–∑–º–µ–Ω–µ–Ω–æ: {task.changed} | –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {task.uploaded}"
        #     )

#############################################################################################
# –§–∞–∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
#############################################################################################

    def process_phase(self):
        # log_section("üõ†Ô∏è –§–∞–∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏", self.log_file)

        if not self.tasks:
            log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
            return

        for task in self.tasks:
            if task.scanned == 0:
                continue

            try:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—ã—Ä—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                try:
                    task.process_raw_value()
                except Exception as e:
                    # log_to_file(self.log_file, f"‚ùå [Task {task.name_of_process}] –û—à–∏–±–∫–∞ –≤ process_raw_value: {e}")
                    continue

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π
                try:
                    task.check_for_update()
                except Exception as e:
                    # log_to_file(self.log_file, f"‚ùå [Task {task.name_of_process}] –û—à–∏–±–∫–∞ –≤ check_for_update: {e}")
                    continue

                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤ –ë–î, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å
                if task.changed:
                    try:
                        update_task_process_fields(self.session, task, self.log_file, table_name="SheetsInfo")
                    except Exception as e:
                        log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –ë–î: {e}")

            except Exception as e:
                log_to_file(self.log_file, f"‚ùå [Task {task.name_of_process}] –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")

        # # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç
        # for task in self.tasks:
        #     log_to_file(
        #         self.log_file,
        #         f"‚ö™ [Task {task.name_of_process}] –û—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ: {task.scanned} | "
        #         f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {task.proceed} | –ò–∑–º–µ–Ω–µ–Ω–æ: {task.changed} | –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {task.uploaded}"
        #     )

#############################################################################################
# –§–∞–∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
#############################################################################################

    def update_phase(self):
        log_section("üîº –§–∞–∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è", self.log_file)

        # --- –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á ---
        tasks_to_update = [t for t in self.tasks if t.values_json and t.changed and t.update_group not in {"update_mistakes_in_db", "feedback_status_update", "update_schedule_OT"}]
        mistakes_to_update = [t for t in self.tasks if t.values_json and t.changed and t.update_group == "update_mistakes_in_db"]
        feedback_to_update = [t for t in self.tasks if t.values_json and t.changed and t.update_group == "feedback_status_update"]
        schedule_OT_to_update = [t for t in self.tasks if t.values_json and t.changed and t.update_group == "update_schedule_OT"]

        # log_to_file(self.log_file, f"üîº –ó–∞–¥–∞—á –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {len(tasks_to_update)}")
        # for task in tasks_to_update:
        #     log_to_file(self.log_file, f"   ‚Ä¢ {task.name_of_process} ({task.update_group})")
        # log_to_file(self.log_file, f"üîº –û—à–∏–±–æ–∫ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {len(mistakes_to_update)}")
        # for task in mistakes_to_update:
        #     log_to_file(self.log_file, f"   ‚Ä¢ {task.name_of_process} ({task.update_group})")
        # log_to_file(self.log_file, f"üîº –§–∏–¥–±–µ–∫–æ–≤ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {len(feedback_to_update)}")
        # for task in feedback_to_update:
        #     log_to_file(self.log_file, f"   ‚Ä¢ {task.name_of_process} ({task.update_group})")
        # log_to_file(self.log_file, f"üîº Schedule OT –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {len(schedule_OT_to_update)}")
        # for task in schedule_OT_to_update:
        #     log_to_file(self.log_file, f"   ‚Ä¢ {task.name_of_process} ({task.update_group})")

        # # --- –û–±—ã—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ ---
        if tasks_to_update:
            try:
                self.import_tasks_to_update(tasks_to_update)
            except Exception as e:
                log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∑–∞–¥–∞—á: {e}")
            time.sleep(3)

        # --- –û—à–∏–±–∫–∏ (MistakeStorage) ---
        if mistakes_to_update:
            try:
                self.import_mistakes_to_update(mistakes_to_update)
            except Exception as e:
                log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –æ—à–∏–±–æ–∫: {e}")
            time.sleep(3)

        # --- –§–∏–¥–±–µ–∫–∏ (FeedbackStorage) ---
        if feedback_to_update:
            try:
                self.import_feedbacks_to_update(feedback_to_update, self.service)
            except Exception as e:
                log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ñ–∏–¥–±–µ–∫–æ–≤: {e}")

        # --- Schedule GP ---
        if schedule_OT_to_update:
            try:
                self.import_schedule_OT_to_update(schedule_OT_to_update)
            except Exception as e:
                log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ schedule OT: {e}")
            time.sleep(3)

        # # --- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–¥–∞—á–∞–º ---
        # for task in self.tasks:
        #     log_to_file(
        #         self.log_file,
        #         f"‚ö™ [Task {task.name_of_process}] –û—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ: {task.scanned} | "
        #         f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {task.proceed} | –ò–∑–º–µ–Ω–µ–Ω–æ: {task.changed} | –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {task.uploaded}"
        #     )

        if not (tasks_to_update or mistakes_to_update or feedback_to_update):
            # log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è.")
            return

        # log_section("üîº –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.", self.log_file)

##############################################################################################
# –ò–º–ø–æ—Ä—Ç –û–±—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á 
##############################################################################################

    def import_tasks_to_update(self, tasks_to_update):
        # log_to_file(self.log_file, f"üîÑ –ù–∞—á–∞–ª–æ —Ñ–∞–∑—ã tasks_to_update. –í—Å–µ–≥–æ –∑–∞–¥–∞—á: {len(tasks_to_update)}")

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∑–∞–¥–∞—á –ø–æ update_group
        tasks_by_group = defaultdict(list)
        for task in tasks_to_update:
            tasks_by_group[task.update_group].append(task)

        for update_group, group_tasks in tasks_by_group.items():
            # log_to_file(self.log_file, f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã: {update_group} ({len(group_tasks)} –∑–∞–¥–∞—á)")

            doc_id = group_tasks[0].target_doc_id
            batch_data = []

            for task in group_tasks:
                if not task.values_json:
                    # log_to_file(self.log_file, f"‚ö™ [Task {task.name_of_process}] –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–ø—É—Å–∫.")
                    continue

                batch_data.append({
                    "range": f"{task.target_page_name}!{task.target_page_area}",
                    "values": task.values_json
                })

            if not batch_data:
                # log_to_file(self.log_file, f"‚ö™ –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è batchUpdate –≥—Ä—É–ø–ø—ã {update_group}.")
                continue
            # for data in batch_data:
            #     print(f"   ‚Ä¢ {data['range']} ({len(data['values'])} —Å—Ç—Ä–æ–∫)")

            # –ü–∞–∫–µ—Ç–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞
            success, error = batch_update(
                service=self.service,
                spreadsheet_id=doc_id,
                batch_data=batch_data,
                token_name=self.token_name,
                update_group=update_group,
                log_file=self.log_file,
                session=self.session  # ‚úÖ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è —Å–µ—Å—Å–∏—è
            )

            if success:
                # log_to_file(self.log_file, f"‚úÖ –ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –¥–ª—è –≥—Ä—É–ø–ø—ã {update_group}")
                for task in group_tasks:
                    task.update_after_upload(success=True)
                    update_task_update_fields(
                        session=self.session,
                        task=task,
                        log_file=self.log_file,
                        table_name="SheetsInfo"
                    )
            else:
                # log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞–∫–µ—Ç–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–µ: {error}")
                # log_to_file(self.log_file, "üîÅ –ü–µ—Ä–µ—Ö–æ–¥ –∫ –æ–¥–∏–Ω–æ—á–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–µ –∑–∞–¥–∞—á")

                for task in group_tasks:
                    if not task.values_json:
                        continue

                    single_data = [{
                        "range": f"{task.target_page_name}!{task.target_page_area}",
                        "values": task.values_json
                    }]

                    single_success, single_error = batch_update(
                        service=self.service,
                        spreadsheet_id=doc_id,
                        batch_data=single_data,
                        token_name=self.token_name,
                        update_group=update_group,
                        log_file=self.log_file,
                        session=self.session
                    )

                    # if single_success:
                    #     log_to_file(self.log_file, f"‚úÖ [Task {task.name_of_process}] –û–±–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ—à—Ç—É—á–Ω–æ.")
                    # else:
                    #     log_to_file(self.log_file, f"‚ùå [Task {task.name_of_process}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏: {single_error}")

                    task.update_after_upload(success=single_success)
                    update_task_update_fields(
                        session=self.session,
                        task=task,
                        log_file=self.log_file,
                        table_name="SheetsInfo"
                    )

###############################################################################################
# –ò–º–ø–æ—Ä—Ç –û—à–∏–±–æ–∫ –≤ –ë–î
###############################################################################################

    @staticmethod

    def parse_date(value):
        try:
            return datetime.strptime(value.strip(), "%d.%m.%Y").date()
        except Exception:
            return None

    @staticmethod
    def parse_time(value):
        try:
            return datetime.strptime(value.strip(), "%H:%M").time()
        except Exception:
            return None
        
    @staticmethod
    def parse_cancel(value):
        return 1 if str(value).strip().lower() == "cancel" else 0


    def import_mistakes_to_update(self, mistakes_to_update):
        # log_to_file(self.log_file, f"üîÑ –ù–∞—á–∞–ª–æ —Ñ–∞–∑—ã mistakes_to_update. –ó–∞–¥–∞—á –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏: {len(mistakes_to_update)}.")
        session = self.session

        try:
            for task in mistakes_to_update:
                sheet = task.raw_values_json
                page_name = task.source_page_name
                floor = get_floor_by_table_name(page_name, FLOORS)
                max_row_in_db = get_max_last_row(session, page_name)

                for row_index, row in enumerate(sheet[1:], start=2):  # –ø—Ä–æ–ø—É—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    if row_index <= max_row_in_db:
                        continue

                    if not row or len(row) < 8:
                        # log_to_file(self.log_file, f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ –ø—É—Å—Ç–∞—è –∏–ª–∏ –Ω–µ–ø–æ–ª–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ {row_index} –∏–∑ {page_name}: {row}")
                        continue

                    try:
                        mistake = MistakeStorage(
                            floor=floor,
                            table_name=page_name,
                            date=self.parse_date(row[0]),
                            time=self.parse_time(row[1]),
                            game_id=row[2],
                            mistake=row[3],
                            type=row[4],
                            is_cancel=self.parse_cancel(row[5]),
                            dealer=row[6],
                            sm=row[7],
                            last_row=row_index
                        )
                        session.add(mistake)

                    except Exception as row_err:
                        log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —Å—Ç—Ä–æ–∫–∏ {row_index} –∏–∑ {page_name}: {row_err}. –°—Ç—Ä–æ–∫–∞: {row}")
                        continue

            session.commit()
            # log_to_file(self.log_file, "‚úÖ –í—Å–µ –æ—à–∏–±–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã.")

        except Exception as task_err:
            session.rollback()
            # log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ mistakes_to_update: {task_err}")

################################################################################################
# –ò–º–ø–æ—Ä—Ç —Å—Ç–∞—Ç—É—Å–∞ —Ñ–∏–¥–±–µ–∫–æ–≤
################################################################################################

    def update_gp_statuses_in_sheet(self, sheets_service):
        session = self.session
        sheet_id = None
        target_range = None

        for task in self.tasks:
            if task.update_group == "feedback_status_update":
                sheet_id = task.target_doc_id
                target_range = f"{task.target_page_name}!{task.target_page_area}"
                break

        if not sheet_id or not target_range:
            # log_to_file(self.log_file, "‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –ª–∏—Å—Ç –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–æ–≤ GP.")
            return []

        try:
            records = session.query(
                FeedbackStorage.gp_name_surname,
                FeedbackStorage.reason,
                FeedbackStorage.forwarded_feedback
            ).filter(FeedbackStorage.gp_name_surname.isnot(None)).all()

            gp_status = {}
            for name, reason, forwarded in records:
                if reason and reason.strip():
                    gp_status[name] = "‚úÖ" if forwarded and forwarded.strip() else "‚ùå"
                    # log_to_file(self.log_file, f"‚úÖ –°—Ç–∞—Ç—É—Å GP: {name} - {gp_status[name]}")
                else:
                    gp_status[name] = "‚ùì"
                    # log_to_file(self.log_file, f"‚ùì –°—Ç–∞—Ç—É—Å GP: {name} - {gp_status[name]}")

            result = sheets_service.spreadsheets().values().get(
                spreadsheetId=sheet_id,
                range=self.doc_id_map.get(sheet_id, target_range),
            ).execute()

            sheet_names = []
            for row in result.get("values", []):
                if isinstance(row, list) and row and isinstance(row[0], str) and row[0].strip():
                    sheet_names.append(row[0].strip())

            # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç—É—Å—ã –≤ FeedbackStatus ---
            for name in sheet_names:
                status = gp_status.get(name, "‚úÖ")  # –¢–µ–ø–µ—Ä—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚úÖ
                existing = session.query(FeedbackStatus).filter_by(name_surname=name).first()
                if existing:
                    existing.status = status
                else:
                    new_status = FeedbackStatus(name_surname=name, status=status)
                    session.add(new_status)
            session.commit()

            output = [[name, gp_status.get(name, "‚úÖ")] for name in sheet_names]  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚úÖ
            # for name in gp_status:
            #     print(f"GP: {name} - {gp_status[name]}")
            # log_to_file(self.log_file, f"‚úÖ –°—Ç–∞—Ç—É—Å—ã GP –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {len(output)}")

            sheets_service.spreadsheets().values().update(
                spreadsheetId=sheet_id,
                range=target_range,
                valueInputOption="RAW",
                body={"values": output}
            ).execute()

            return output

        except Exception as e:
            session.rollback()
            log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ GP —Å—Ç–∞—Ç—É—Å–æ–≤: {e}")
            return []

    @staticmethod
    def safe_int(value):
        try:
            return int(value)
        except (ValueError, TypeError):
            return None

    def import_feedbacks_to_update(self, feedback_to_update, sheets_service):
        session = self.session

        try:
            for task in feedback_to_update:
                sheet = task.raw_values_json
                page_name = task.target_page_name
                empty_row_streak = 0

                for row_index, row in enumerate(sheet[1:], start=2):
                    if not row or not str(row[0]).isdigit():
                        continue

                    feedback_id = int(row[0])
                    data = row[1:]

                    expected_len = 13
                    data = (data + [None] * expected_len)[:expected_len]

                    essential_fields = data[:8] + data[9:]
                    if all((str(f or '').strip() == '') for f in essential_fields):
                        empty_row_streak += 1
                    else:
                        empty_row_streak = 0

                    if empty_row_streak >= 15:
                        break

                    try:
                        existing = session.query(FeedbackStorage).filter_by(id=feedback_id).first()
                        if existing:
                            for attr, val in zip([
                                "date", "shift", "floor", "game", "gp_name_surname",
                                "sm_name_surname", "reason", "total", "proof",
                                "explanation_of_the_reason", "action_taken",
                                "forwarded_feedback", "comment_after_forwarding"
                            ], data):
                                if attr == "total":
                                    val = self.safe_int(val)
                                elif attr == "date":
                                    val = self.parse_date(val)
                                setattr(existing, attr, val)
                        else:
                            new_feedback = FeedbackStorage(
                                id=feedback_id,
                                date=self.parse_date(data[0]), 
                                shift=data[1], 
                                floor=data[2], 
                                game=data[3],
                                gp_name_surname=data[4], 
                                sm_name_surname=data[5], 
                                reason=data[6],
                                total=self.safe_int(data[7]), 
                                proof=data[8], 
                                explanation_of_the_reason=data[9],
                                action_taken=data[10], 
                                forwarded_feedback=data[11],
                                comment_after_forwarding=data[12]
                            )
                            session.add(new_feedback)

                    except Exception as row_err:
                        log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–æ–∫–∏ {row_index} –∏–∑ {page_name}: {row_err}. –°—Ç—Ä–æ–∫–∞: {row}")

            session.commit()
        except Exception as e:
            session.rollback()
            # log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ —Ñ–∏–¥–±–µ–∫–æ–≤: {e}")
        finally:
            self.update_gp_statuses_in_sheet(sheets_service)
            # log_to_file(self.log_file, "üîÑ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ñ–∞–∑—ã feedback_status_update.")


###############################################################################################
# –ò–º–ø–æ—Ä—Ç Schedule OT
################################################################################################

    def import_schedule_OT_to_update(self, tasks):
        if not tasks:
            log_to_file(self.log_file, "‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á –≤ import_schedule_OT_to_update")
            return

        task = tasks[0]
        spreadsheet_id = task.source_doc_id
        values = task.values_json
        session = self.session

        # –ü–æ–ª—É—á–∞–µ–º valid_from
        tracked = session.query(TrackedTables).filter_by(spreadsheet_id=spreadsheet_id).first()
        if not tracked or not tracked.valid_from:
            # log_to_file(self.log_file, f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω valid_from –¥–ª—è spreadsheet_id={spreadsheet_id}")
            return

        valid_from = tracked.valid_from
        related_month = valid_from.replace(day=1)

        if not values or len(values) == 0:
            # log_to_file(self.log_file, "‚ùå values_json –ø—É—Å—Ç –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω")
            return

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏
        existing_records = session.query(ScheduleOT).filter_by(related_month=related_month).all()
        existing_lookup = {
            (rec.dealer_name.strip(), rec.date): rec for rec in existing_records
        }

        new_entries = 0
        updated_entries = 0

        for row in values:
            if not row or len(row) < 2:
                continue

            dealer_name = row[0]
            if not dealer_name or not isinstance(dealer_name, str):
                continue

            dealer_name = dealer_name.strip()

            for col_idx, shift in enumerate(row[1:], start=1):  # 1-based index = day number
                if not shift or not isinstance(shift, str) or shift.strip() in {"", "-", "/"}:
                    continue

                try:
                    shift_date = related_month.replace(day=col_idx)
                except ValueError:
                    continue  # –Ω–∞–ø—Ä–∏–º–µ—Ä, –µ—Å–ª–∏ –¥–µ–Ω—å > 31

                shift_type = shift.strip()
                key = (dealer_name, shift_date)

                if key in existing_lookup:
                    record = existing_lookup[key]
                    if record.shift_type != shift_type:
                        record.shift_type = shift_type
                        updated_entries += 1
                else:
                    new_record = ScheduleOT(
                        date=shift_date,
                        dealer_name=dealer_name,
                        shift_type=shift_type,
                        related_month=related_month
                    )
                    session.add(new_record)
                    new_entries += 1

        session.commit()

        log_to_file(
            self.log_file,
            f"‚úÖ ScheduleOT: {related_month.strftime('%B %Y')} ‚Äî "
            f"{new_entries} –Ω–æ–≤—ã—Ö, {updated_entries} –æ–±–Ω–æ–≤–ª–µ–Ω–æ."
        )
