# scanners/sheetsinfo_scanner.py

import time
from datetime import datetime, timedelta, date
from collections import defaultdict
import traceback
import calendar
import json
from typing import Optional

from tg_bot.utils.settings_access import is_scanner_enabled
from core.data import load_sheetsinfo_tasks
from utils.logger import (
    log_info, log_success, log_warning, log_error, log_section, log_separator
)
from utils.db_orm import get_max_last_row
from utils.floor_resolver import get_floor_by_table_name
from database.session import get_session
# –í–≤–µ—Ä—Ö—É —Ñ–∞–π–ª–∞:
from core.data import refresh_materialized_views

from database.db_models import GamingTable, ScheduleOT

from core.config import (
    SHEETSINFO_LOG,
    SHEETINFO_INTERVAL,
    FLOORS
)
from utils.db_orm import (
    update_task_scan_fields,
    update_task_process_fields,
    update_task_update_fields
)
from utils.utils import (
    load_credentials,
    check_sheet_exists,
    batch_get,
    batch_update,
)
from .sheetsinfo_imports import (
    import_mistakes_to_update,
    import_feedbacks_to_update,
    import_qa_list_to_update,
)

class SheetsInfoScanner:  
    """
    SheetsInfoScanner ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è, –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∑–∞–¥–∞—á –ø–æ Google Sheets.
    """

    def __init__(self, token_map, log_file=None):
        from core.config import SHEETSINFO_LOG
        self.token_map = token_map
        self.log_file = log_file if log_file else (SHEETSINFO_LOG if SHEETSINFO_LOG else "logs/scanner_sheetsinfo.log")
        self.tasks = []

    def run(self):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ä–∞–±–æ—Ç—ã —Å–∫–∞–Ω–µ—Ä–∞: –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–¥–∞—á, —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –æ–±—Ä–∞–±–æ—Ç–∫–∞, –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ.
        –ö–∞–∂–¥–∞—è —Ñ–∞–∑–∞ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å–µ—Å—Å–∏–∏, —á—Ç–æ–±—ã –æ—à–∏–±–∫–∏ –Ω–µ –≤–ª–∏—è–ª–∏ –Ω–∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∞–∑—ã.
        """
        while True:
            if not is_scanner_enabled("sheets_scanner"):
                time.sleep(10)
                continue
            try:
                log_separator(self.log_file, "run")
                log_section(self.log_file, "run", "‚ñ∂Ô∏è SheetsInfo –ê–∫—Ç–∏–≤–µ–Ω. –ù–æ–≤—ã–π —Ü–∏–∫–ª —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è\n")

                token_name = list(self.token_map.keys())[0]
                token_path = self.token_map[token_name]
                self.token_name = token_name

                # –°–µ—Ä–≤–∏—Å –∏ doc_id_map –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –æ–¥–∏–Ω —Ä–∞–∑
                with get_session() as session:
                    self.service = load_credentials(token_path, self.log_file)
                    log_info(self.log_file, "run", None, "token", f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–∫–µ–Ω: {self.token_name}")
                    from core.data import return_tracked_tables
                    self.doc_id_map = return_tracked_tables(session)

                # –ö–∞–∂–¥–∞—è —Ñ–∞–∑–∞ ‚Äî –æ—Ç–¥–µ–ª—å–Ω–∞—è —Å–µ—Å—Å–∏—è
                for phase_name, method in [
                    ("load_tasks", self.load_tasks),
                    ("scan_phase", self.scan_phase),
                    ("process_phase", self.process_phase),
                    ("update_phase", self.update_phase),
                ]:
                    log_separator(self.log_file, phase_name)
                    try:
                        log_info(self.log_file, phase_name, None, "start", f"–°—Ç–∞—Ä—Ç —ç—Ç–∞–ø–∞ {phase_name}")
                        with get_session() as session:
                            method(session)
                        log_success(self.log_file, phase_name, None, "finish", f"–≠—Ç–∞–ø {phase_name} –∑–∞–≤–µ—Ä—à—ë–Ω\n")
                    except Exception as e:
                        log_error(self.log_file, phase_name, None, "fail", f"–û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ {phase_name}", exc=e)
                        # –ù–µ –ø—Ä–µ—Ä—ã–≤–∞–µ–º —Ü–∏–∫–ª, –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É

            except Exception as e:
                log_error(self.log_file, "run", None, "fail", "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ", exc=e)
                time.sleep(10)

            # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —Ü–∏–∫–ª–∞–º–∏ (–Ω–µ –º–µ–Ω–µ–µ 3 —Å–µ–∫—É–Ω–¥)
            interval = max(SHEETINFO_INTERVAL, 3)
            time.sleep(interval)

#############################################################################################
# –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–¥–∞—á –∏–∑ –ë–î
#############################################################################################

    def load_tasks(self, session):
        log_section(self.log_file, "load_tasks", "üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–¥–∞—á –∏–∑ SheetsInfo")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å –ø–µ—Ä–µ–¥–∞—á–µ–π doc_id_map
        self.tasks = load_sheetsinfo_tasks(session, self.log_file)

        if not self.tasks:
            log_info(self.log_file, "load_tasks", None, "empty", "–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è")
            self.tasks = []
            return

        skipped = 0
        for task in self.tasks:
            try:
                ok = task.assign_doc_ids(self.doc_id_map, self.log_file)
                if not ok:
                    skipped += 1
                    log_warning(self.log_file, "load_tasks", task.name_of_process, "skipped", "–ù–µ—Ç doc_id, –∑–∞–¥–∞—á–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∞")
            except Exception as e:
                skipped += 1
                log_error(self.log_file, "load_tasks", task.name_of_process, "fail", "–û—à–∏–±–∫–∞ –≤ assign_doc_ids", exc=e)

        log_info(self.log_file, "load_tasks", None, "done",
                f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–¥–∞—á: {len(self.tasks)}, –ø—Ä–æ–ø—É—â–µ–Ω–æ –±–µ–∑ doc_id: {skipped}")

#############################################################################################
# –§–∞–∑–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
#############################################################################################

    def scan_phase(self, session):
        log_section(self.log_file, "scan_phase", "üîç –§–∞–∑–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è")

        if not self.tasks:
            log_info(self.log_file, "scan_phase", None, "empty", "–ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è")
            return

        # üß© –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ scan_group
        scan_groups = defaultdict(list)
        for task in self.tasks:
            scan_groups[task.scan_group].append(task)

        # üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã
        for scan_group, group_tasks in scan_groups.items():
            log_section(self.log_file, "scan_phase", f"\nüóÇÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ scan_group: {scan_group} ({len(group_tasks)} –∑–∞–¥–∞—á)\n")
            if not group_tasks:
                continue

            doc_id = group_tasks[0].source_doc_id
            unique_sheet_names = set(task.source_page_name for task in group_tasks)

            # ‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ª–∏—Å—Ç–æ–≤ (–ø–µ—Ä–≤–∞—è —Ä–∞–±–æ—Ç–∞ —Å API)
            exists_map = {
                sheet_name: check_sheet_exists(self.service, doc_id, sheet_name, self.log_file, self.token_name)
                for sheet_name in unique_sheet_names
            }

            for sheet_name, exists in exists_map.items():
                log_info(self.log_file, "scan_phase", None, "sheet_exists", f"–õ–∏—Å—Ç '{sheet_name}' {'—Å—É—â–µ—Å—Ç–≤—É–µ—Ç' if exists else '–Ω–µ –Ω–∞–π–¥–µ–Ω'}")

            # üßæ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤–∞–ª–∏–¥–Ω—ã—Ö –∑–∞–¥–∞—á
            valid_tasks = []
            for task in group_tasks:
                if exists_map.get(task.source_page_name):
                    valid_tasks.append(task)
                else:
                    log_warning(self.log_file, "scan_phase", task.name_of_process, "skipped", f"–õ–∏—Å—Ç '{task.source_page_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    task.update_after_scan(success=False)
                    update_task_scan_fields(session, task, self.log_file, table_name="SheetsInfo")

            if not valid_tasks:
                log_info(self.log_file, "scan_phase", None, "empty", f"–í—Å–µ –∑–∞–¥–∞—á–∏ –≥—Ä—É–ø–ø—ã {scan_group} –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã. –ü—Ä–æ–ø—É—Å–∫ batchGet.")
                continue

            # üîó –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º
            range_to_tasks = defaultdict(list)
            for task in valid_tasks:
                range_str = f"{task.source_page_name}!{task.source_page_area}"
                range_to_tasks[range_str].append(task)

            ranges = list(range_to_tasks.keys())
            log_info(self.log_file, "scan_phase", None, "batch_get", f"–û—Ç–ø—Ä–∞–≤–∫–∞ batchGet –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç {task.source_table_type} —Å {len(ranges)} –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏")

            # üì• batchGet –∑–∞–ø—Ä–æ—Å
            response_data = batch_get(
                self.service,
                doc_id,
                ranges,
                scan_group,
                self.log_file,
                self.token_name
            )

            # ‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ ‚Äî –≤—Å–µ –∑–∞–¥–∞—á–∏ failed
            if not response_data:
                for task in valid_tasks:
                    task.update_after_scan(success=False)
                    update_task_scan_fields(session, task, self.log_file, table_name="SheetsInfo")
                log_warning(self.log_file, "scan_phase", None, "empty", "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç batchGet. –í—Å–µ –∑–∞–¥–∞—á–∏ –±—É–¥—É—Ç –æ—Ç–º–µ—á–µ–Ω—ã –∫–∞–∫ –Ω–µ—É–¥–∞—á–Ω—ã–µ.")
                continue

            # ‚úÖ –û—á–∏—Å—Ç–∫–∞ –∫–ª—é—á–µ–π –æ—Ç –∫–∞–≤—ã—á–µ–∫
            normalized_response = {}
            for k, v in response_data.items():
                clean_key = k.replace("'", "")
                if "!" in clean_key:
                    sheet_name, cells_range = clean_key.split("!", 1)
                    normalized_response[(sheet_name.strip(), cells_range.strip())] = v

            # üì§ –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –∑–∞–¥–∞—á–∞–º
            for task in valid_tasks:
                expected_sheet = task.source_page_name.strip()
                expected_area_start = task.source_page_area.split(":")[0].strip()
                matched_values = None

                for (sheet_name, cells_range), values in normalized_response.items():
                    if sheet_name == expected_sheet and cells_range.startswith(expected_area_start):
                        matched_values = values
                        break

                if matched_values:
                    task.raw_values_json = matched_values
                    task.update_after_scan(success=True)
                    update_task_scan_fields(session, task, self.log_file, table_name="SheetsInfo")
                    log_success(self.log_file, "scan_phase", task.name_of_process, "found", f"–ù–∞–π–¥–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω {sheet_name}!{cells_range}, —Å—Ç—Ä–æ–∫: {len(matched_values)}")
                else:
                    task.update_after_scan(success=False)
                    update_task_scan_fields(session, task, self.log_file, table_name="SheetsInfo")
                    log_warning(self.log_file, "scan_phase", task.name_of_process, "not_found", f"–î–∏–∞–ø–∞–∑–æ–Ω {expected_sheet}!{task.source_page_area} –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç.")

        # üßæ –§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á—ë—Ç
        log_info(self.log_file, "scan_phase", None, "summary", "\n".join(
            [f"‚Ä¢ {task.name_of_process} {task.source_page_name}: scanned={task.scanned}, processed={task.proceed}, changed={task.changed}, uploaded={task.uploaded}"
            for task in self.tasks]
        ) + "\n")

        log_success(self.log_file, "scan_phase", None, "finish", "–§–∞–∑–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n")

#############################################################################################
# –§–∞–∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
#############################################################################################

    def process_phase(self, session):
        log_section(self.log_file, "process_phase", "üõ†Ô∏è –§–∞–∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        if not self.tasks:
            log_info(self.log_file, "process_phase", None, "empty", "–ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
            return
        for task in self.tasks:
            if task.scanned == 0:
                continue
            try:
                try:
                    task.process_raw_value(self.log_file)
                except Exception as e:
                    log_error(self.log_file, "process_phase", task.name_of_process, "fail", "–û—à–∏–±–∫–∞ –≤ process_raw_value", exc=e)
                    continue
                try:
                    task.check_for_update()
                except Exception as e:
                    log_error(self.log_file, "process_phase", task.name_of_process, "fail", "–û—à–∏–±–∫–∞ –≤ check_for_update", exc=e)
                    continue
                if task.changed:
                    try:
                        update_task_process_fields(session, task, self.log_file, table_name="SheetsInfo")
                        session.commit()  # –∫–æ–º–º–∏—Ç–∏–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ –∑–∞–¥–∞—á–µ
                        log_success(self.log_file, "process_phase", task.name_of_process, "changed", "–î–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
                    except Exception as e:
                        session.rollback()  # –æ—Ç–∫–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø–æ —ç—Ç–æ–π –∑–∞–¥–∞—á–µ
                        log_error(self.log_file, "process_phase", task.name_of_process, "fail", "–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –ë–î", exc=e)
            except Exception as e:
                log_error(self.log_file, "process_phase", task.name_of_process, "fail", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ", exc=e)
        log_info(self.log_file, "process_phase", None, "summary", "\n".join(
            [f"‚Ä¢ {task.name_of_process} {task.source_page_name}: scanned={task.scanned}, processed={task.proceed}, changed={task.changed}, uploaded={task.uploaded}"
            for task in self.tasks]
        ) + "\n")
        log_success(self.log_file, "process_phase", None, "finish", "–§–∞–∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n")

#############################################################################################
# –§–∞–∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
#############################################################################################

    def update_phase(self, session):
        log_section(self.log_file, "update_phase", "üîº –§–∞–∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")

        grouped_tasks = defaultdict(list)
        updated_groups = set()

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∑–∞–¥–∞—á –ø–æ update_group
        for t in self.tasks:
            if not (t.values_json and t.changed):
                log_warning(self.log_file, "update_phase", t.name_of_process, "skipped", f"–ü—Ä–æ–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ {t.name_of_process}: –Ω–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –∏–ª–∏ –ø—É—Å—Ç–æ–π values_json")
                continue
            grouped_tasks[t.update_group].append(t)

        # –û—Å–Ω–æ–≤–Ω–æ–π –∏–º–ø–æ—Ä—Ç ‚Äî –æ–¥–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π
        for group_name, tasks in grouped_tasks.items():
            time.sleep(5)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏ API
            log_info(self.log_file, "update_phase", None, group_name, f"üîº –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ {group_name}: {len(tasks)}")
            try:
                self.import_tasks_to_update(tasks, session)
                updated_groups.add(group_name)
            except Exception as e:
                session.rollback()
                log_error(self.log_file, "update_phase", None, f"{group_name}_fail", f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –≥—Ä—É–ø–ø—ã: {e}")
            time.sleep(3)

        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤—å—é
        refresh_materialized_views(session, updated_groups, self.log_file)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        log_info(self.log_file, "update_phase", None, "summary", "üîº –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–¥–∞—á–∞–º:")
        for task in self.tasks:
            log_info(
                self.log_file,
                "update_phase",
                task.name_of_process,
                "summary",
                f"‚ö™ [Task {task.name_of_process} {task.source_page_name} {task.related_month}] "
                f"–û—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ: {task.scanned} | –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {task.proceed} | "
                f"–ò–∑–º–µ–Ω–µ–Ω–æ: {task.changed} | –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {task.uploaded}"
            )

        log_section(self.log_file, "update_phase", "üîº –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.\n")

##############################################################################################
# –ò–º–ø–æ—Ä—Ç –û–±—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á 
##############################################################################################

    def import_tasks_to_update(self, tasks_to_update, session):
        log_info(self.log_file, "import_tasks_to_update", None, "start", f"üîÑ –ù–∞—á–∞–ª–æ —Ñ–∞–∑—ã tasks_to_update. –í—Å–µ–≥–æ –∑–∞–¥–∞—á: {len(tasks_to_update)}")

        tasks_by_group = defaultdict(list)
        for task in tasks_to_update:
            tasks_by_group[task.update_group].append(task)

        for update_group, group_tasks in tasks_by_group.items():
            log_info(self.log_file, "import_tasks_to_update", None, "group", f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã: {update_group} ({len(group_tasks)} –∑–∞–¥–∞—á)")

            doc_ids = set(t.target_doc_id for t in group_tasks)
            if len(doc_ids) != 1:
                log_error(self.log_file, "import_tasks_to_update", None, "multi_doc_id", f"‚ùå –í –≥—Ä—É–ø–ø–µ {update_group} –Ω–µ—Å–∫–æ–ª—å–∫–æ doc_id: {doc_ids}. –ü—Ä–æ–ø—É—Å–∫.")
                continue
            doc_id = doc_ids.pop()

            valid_tasks = self._build_batch_data(group_tasks, session)
            if not valid_tasks:
                log_warning(self.log_file, "import_tasks_to_update", None, "no_data", f"‚ö†Ô∏è –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è batchUpdate –≥—Ä—É–ø–ø—ã {update_group}. –ü—Ä–æ–ø—É—Å–∫.")
                continue

            batch_data = [{
                "range": f"{task.target_page_name}!{task.target_page_area}",
                "values": values
            } for task, values in valid_tasks.items()]

            try:
                success, error = self._try_batch_update(doc_id, batch_data, update_group)
                if success:
                    log_success(self.log_file, "import_tasks_to_update", None, "batch_update", f"‚úÖ –ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –¥–ª—è –≥—Ä—É–ø–ø—ã {update_group}")
                    self._commit_task_updates(valid_tasks.keys(), session, success=True)
                else:
                    log_error(self.log_file, "import_tasks_to_update", None, "batch_update_fail", f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞–∫–µ—Ç–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–µ: {error}")
                    self._fallback_single_upload(valid_tasks, doc_id, update_group, session)
            except Exception as e:
                log_error(self.log_file, "import_tasks_to_update", None, "batch_update_exception", f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ batch_update: {e}")
                self._fallback_single_upload(valid_tasks, doc_id, update_group, session)

    def _convert_jsonb_to_tabular(self, jsonb_data: list) -> list:
        """
        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π (JSONB) –≤ —Ç–∞–±–ª–∏—Ü—É (—Å–ø–∏—Å–æ–∫ —Å–ø–∏—Å–∫–æ–≤),
        –≥–¥–µ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ ‚Äî –∑–∞–≥–æ–ª–æ–≤–∫–∏, –∞ –ø–æ—Å–ª–µ–¥—É—é—â–∏–µ ‚Äî –∑–Ω–∞—á–µ–Ω–∏—è.
        """
        if not jsonb_data or not isinstance(jsonb_data, list):
            return []

        if not isinstance(jsonb_data[0], dict):
            raise ValueError("–û–∂–∏–¥–∞–ª—Å—è —Å–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π")

        headers = list(jsonb_data[0].keys())
        table = [headers]

        for row in jsonb_data:
            table.append([row.get(h, "") for h in headers])

        return table

    def _build_batch_data(self, tasks, session):
        valid_tasks = {}
        for task in tasks:
            try:
                raw = json.loads(task.values_json) if isinstance(task.values_json, str) else task.values_json
                values = self._convert_jsonb_to_tabular(raw)
                if not isinstance(values, list):
                    raise ValueError("JSONB –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Å–ø–∏—Å–∫–æ–º —Å–ø–∏—Å–∫–æ–≤")
                valid_tasks[task] = values
            except Exception as e:
                log_error(self.log_file, "import_tasks_to_update", task.name_of_process, "json_decode_error", f"‚ùå –ù–µ–≤–∞–ª–∏–¥–Ω—ã–π JSONB: {e}")
                try:
                    task.update_after_upload(success=False)
                    update_task_update_fields(session=session, task=task, log_file=self.log_file, table_name="SheetsInfo")
                    session.commit()
                except Exception as inner:
                    session.rollback()
                    log_error(self.log_file, "import_tasks_to_update", task.name_of_process, "db_flag_fail", f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–º–µ—Ç–∏—Ç—å –∑–∞–¥–∞—á—É –∫–∞–∫ failed: {inner}")
        return valid_tasks

    def _fallback_single_upload(self, valid_tasks, doc_id, update_group, session):
        for task, values in valid_tasks.items():
            try:
                single_data = [{
                    "range": f"{task.target_page_name}!{task.target_page_area}",
                    "values": values
                }]
                success, error = self._try_batch_update(doc_id, single_data, update_group)
                if success:
                    log_success(self.log_file, "import_tasks_to_update", task.name_of_process, "single_update", f"‚úÖ [Task {task.name_of_process} {task.source_page_name} {task.related_month}] –û–±–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ—à—Ç—É—á–Ω–æ.")
                else:
                    log_error(self.log_file, "import_tasks_to_update", task.name_of_process, "single_update_fail", f"‚ùå [Task {task.name_of_process} {task.source_page_name} {task.related_month}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏: {error}")

                task.update_after_upload(success=success)
                update_task_update_fields(session=session, task=task, log_file=self.log_file, table_name="SheetsInfo")
                session.commit()
            except Exception as e:
                session.rollback()
                log_error(self.log_file, "import_tasks_to_update", task.name_of_process, "single_update_exception", f"‚ùå –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –ø–æ—à—Ç—É—á–Ω–æ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏: {e}")

    def _try_batch_update(self, doc_id, batch_data, update_group, retries=3):
        last_error = None
        for attempt in range(retries):
            try:
                return batch_update(
                    service=self.service,
                    spreadsheet_id=doc_id,
                    batch_data=batch_data,
                    token_name=self.token_name,
                    update_group=update_group,
                    log_file=self.log_file
                )
            except Exception as e:
                last_error = e
                time.sleep(2 ** attempt)
        return False, last_error

    def _commit_task_updates(self, tasks, session, success):
        try:
            for task in tasks:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å task –ø–µ—Ä–µ–¥ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º
                if not hasattr(task, "update_after_upload") or not hasattr(task, "name_of_process"):
                    log_error(self.log_file, "import_tasks_to_update", None, "invalid_task", f"‚ùå –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ–±—ä–µ–∫—Ç –∑–∞–¥–∞—á–∏: {repr(task)}")
                    continue
                task.update_after_upload(success=success)
                update_task_update_fields(session=session, task=task, log_file=self.log_file, table_name="SheetsInfo")
            session.commit()
        except Exception as e:
            session.rollback()
            log_error(self.log_file, "import_tasks_to_update", None, "db_update_fail", f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á: {e}")