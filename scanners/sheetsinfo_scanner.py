# scanners/sheetsinfo_scanner.py

import time
from datetime import datetime, timedelta
from collections import defaultdict

from tg_bot.utils.settings_access import is_scanner_enabled
from core.data import load_sheetsinfo_tasks
from utils.logger import log_to_file, log_separator, log_section
from utils.floor_resolver import get_floor_by_table_name
from database.session import SessionLocal

from database.db_models import MistakeStorage, FeedbackStorage,  ScheduleOT, TrackedTables
from utils.db_orm import get_max_last_row

from core.config import (
    SHEETSINFO_LOG,
    SHEETINFO_INTERVAL,
    FLOORS
)
from utils.db_orm import (
    update_task_scan_fields,
    update_task_process_fields,
    update_task_update_fields
)
from utils.utils import (
    load_credentials,
    check_sheet_exists,
    batch_get,
    batch_update,
)

class SheetsInfoScanner:
    def __init__(self, token_map, doc_id_map):
        self.token_map = token_map
        self.doc_id_map = doc_id_map
        self.log_file = SHEETSINFO_LOG
        self.tasks = []

    def run(self):
        while True:
            try:
                if not is_scanner_enabled("sheets_scanner"):
                    time.sleep(10)
                    continue

                log_section("‚ñ∂Ô∏è SheetsInfo –ê–∫—Ç–∏–≤–µ–Ω. –ù–æ–≤—ã–π —Ü–∏–∫–ª —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", self.log_file)

                token_name = list(self.token_map.keys())[0]
                token_path = self.token_map[token_name]
                self.token_name = token_name

                with SessionLocal() as session:
                    self.service = load_credentials(token_path, self.log_file)
                    log_to_file(self.log_file, f"üîê –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–∫–µ–Ω: {self.token_name}")

                    for phase_name, method in [
                        ("–∑–∞–≥—Ä—É–∑–∫–∏ –∑–∞–¥–∞—á", lambda: self.load_tasks(session)),
                        ("—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", lambda: self.scan_phase(session)),
                        ("–æ–±—Ä–∞–±–æ—Ç–∫–∏", lambda: self.process_phase(session)),
                        ("–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è", lambda: self.update_phase(session))
                    ]:
                        try:
                            method()
                        except Exception as e:
                            log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ {phase_name}: {e}")
                            raise

            except Exception as e:
                log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ SheetsInfo: {e}")
                time.sleep(10)

            time.sleep(SHEETINFO_INTERVAL)

#############################################################################################
# –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–¥–∞—á –∏–∑ –ë–î
#############################################################################################

    def load_tasks(self, session):
        log_section("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–¥–∞—á –∏–∑ SheetsInfo", self.log_file)

        self.tasks = load_sheetsinfo_tasks(session, self.log_file)

        if not self.tasks:
            # log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.")
            self.tasks = []
            return

        # log_to_file(self.log_file, f"üîÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–¥–∞—á: {len(self.tasks)}")

        skipped = 0
        for task in self.tasks:
            if not task.assign_doc_ids(self.doc_id_map):
                skipped += 1

        # if skipped:
        #     log_to_file(self.log_file, f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –∑–∞–¥–∞—á –±–µ–∑ doc_id: {skipped}")

#############################################################################################
# –§–∞–∑–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
#############################################################################################

    def scan_phase(self, session):
        # log_section("üîç –§–∞–∑–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", self.log_file)

        if not self.tasks:
            # log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.")
            return

        ready_tasks = [task for task in self.tasks if task.is_ready_to_scan()]
        if not ready_tasks:
            # log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á, –≥–æ—Ç–æ–≤—ã—Ö –∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é.")
            return

        # log_to_file(self.log_file, f"üîé –ù–∞–π–¥–µ–Ω–æ {len(ready_tasks)} –∑–∞–¥–∞—á, –≥–æ—Ç–æ–≤—ã—Ö –∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é:")

        scan_groups = defaultdict(list)
        for task in ready_tasks:
            if not task.assign_doc_ids(self.doc_id_map):
                # log_to_file(self.log_file, f"‚ö†Ô∏è [Task {task.name_of_process} {task.source_page_name}] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å doc_id. –ü—Ä–æ–ø—É—Å–∫.")
                continue
            scan_groups[task.scan_group].append(task)

        for scan_group, group_tasks in scan_groups.items():
            # log_separator(self.log_file)
            # log_to_file(self.log_file, f"üìò –û–±—Ä–∞–±–æ—Ç–∫–∞ scan_group: {scan_group} ({len(group_tasks)} –∑–∞–¥–∞—á)")

            if not group_tasks:
                # log_to_file(self.log_file, "‚ö™ –í –≥—Ä—É–ø–ø–µ –Ω–µ—Ç –∑–∞–¥–∞—á.")
                continue

            doc_id = group_tasks[0].source_doc_id
            unique_sheet_names = set(task.source_page_name for task in group_tasks)
            # log_to_file(self.log_file, f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ª–∏—Å—Ç–æ–≤: {unique_sheet_names}")

            exists_map = {
                sheet_name: check_sheet_exists(self.service, doc_id, sheet_name, self.log_file, self.token_name)
                for sheet_name in unique_sheet_names
            }

            # for sheet_name, exists in exists_map.items():
            #     log_to_file(self.log_file, f"{'‚úÖ' if exists else '‚ö†Ô∏è'} –õ–∏—Å—Ç '{sheet_name}' {'—Å—É—â–µ—Å—Ç–≤—É–µ—Ç' if exists else '–Ω–µ –Ω–∞–π–¥–µ–Ω'}.")

            valid_tasks = []
            for task in group_tasks:
                sheet_name = task.source_page_name
                if exists_map.get(sheet_name):
                    # log_to_file(self.log_file, f"‚û°Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º '{sheet_name}' –¥–ª—è –∑–∞–¥–∞—á–∏ {task.name_of_process}.")
                    valid_tasks.append(task)
                else:
                    # log_to_file(self.log_file, f"‚õî –ü—Ä–æ–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ {task.name_of_process}: –ª–∏—Å—Ç '{sheet_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω.")
                    task.update_after_scan(success=False)
                    update_task_scan_fields(session, task, self.log_file, table_name="SheetsInfo")

            if not valid_tasks:
                # log_to_file(self.log_file, f"‚ö™ –í—Å–µ –∑–∞–¥–∞—á–∏ –≥—Ä—É–ø–ø—ã {scan_group} –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã. –ü—Ä–æ–ø—É—Å–∫ batchGet.")
                continue

            range_to_tasks = defaultdict(list)
            for task in valid_tasks:
                range_str = f"{task.source_page_name}!{task.source_page_area}"
                range_to_tasks[range_str].append(task)

            ranges = list(range_to_tasks.keys())

            log_to_file(self.log_file, "")
            log_to_file(self.log_file, f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ batchGet –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç {task.source_table_type} —Å {len(ranges)} —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏:")
            
            for r in ranges:
                log_to_file(self.log_file, f"   ‚Ä¢ {r}")

            response_data = batch_get(
                self.service,
                doc_id,
                ranges,
                scan_group,
                self.log_file,
                self.token_name
            )
            if not response_data:
                # log_to_file(self.log_file, "‚ùå –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç batchGet. –í—Å–µ –∑–∞–¥–∞—á–∏ –±—É–¥—É—Ç –æ—Ç–º–µ—á–µ–Ω—ã –∫–∞–∫ –Ω–µ—É–¥–∞—á–Ω—ã–µ.")
                for task in valid_tasks:
                    task.update_after_scan(success=False)
                    update_task_scan_fields(session, task, self.log_file, table_name="SheetsInfo")
                continue

            normalized_response = {}
            for k, v in response_data.items():
                clean_key = k.replace("'", "")
                if "!" in clean_key:
                    sheet_name, cells_range = clean_key.split("!", 1)
                    normalized_response[(sheet_name.strip(), cells_range.strip())] = v

            # log_to_file(self.log_file, "")
            # log_to_file(self.log_file, f"üì• –ü–æ–ª—É—á–µ–Ω—ã –¥–∏–∞–ø–∞–∑–æ–Ω—ã: {list(normalized_response.keys())}")

            for task in valid_tasks:
                expected_sheet = task.source_page_name.strip()
                expected_area_start = task.source_page_area.split(":")[0].strip()
                matched_values = None

                for (sheet_name, cells_range), values in normalized_response.items():
                    if sheet_name == expected_sheet and cells_range.startswith(expected_area_start):
                        matched_values = values
                        break

                if matched_values:
                    task.raw_values_json = matched_values
                    task.update_after_scan(success=True)
                    update_task_scan_fields(session, task, self.log_file, table_name="SheetsInfo")
                    log_to_file(self.log_file, f"‚úÖ [Task {task.name_of_process} {task.source_page_name}] –ù–∞–π–¥–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω {sheet_name}!{cells_range}, —Å—Ç—Ä–æ–∫: {len(matched_values)}")
                else:
                    task.update_after_scan(success=False)
                    update_task_scan_fields(session, task, self.log_file, table_name="SheetsInfo")
                    log_to_file(self.log_file, f"‚ö†Ô∏è [Task {task.name_of_process} {task.source_page_name}] –î–∏–∞–ø–∞–∑–æ–Ω {expected_sheet}!{task.source_page_area} –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç.")

        for task in self.tasks:
            log_to_file(
                self.log_file,
                f"‚ö™ [Task {task.name_of_process} {task.source_page_name}] –û—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ: {task.scanned} | "
                f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {task.proceed} | –ò–∑–º–µ–Ω–µ–Ω–æ: {task.changed} | –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {task.uploaded}"
            )
        log_to_file(self.log_file, "üîç –§–∞–∑–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

#############################################################################################
# –§–∞–∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
#############################################################################################

    def process_phase(self, session):
        log_section("üõ†Ô∏è –§–∞–∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏", self.log_file)

        if not self.tasks:
            log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
            return

        for task in self.tasks:
            if task.scanned == 0:
                continue

            try:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—ã—Ä—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                try:
                    task.process_raw_value()
                except Exception as e:
                    log_to_file(self.log_file, f"‚ùå [Task {task.name_of_process} {task.source_page_name}] –û—à–∏–±–∫–∞ –≤ process_raw_value: {e}")
                    continue

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π
                try:
                    task.check_for_update()
                except Exception as e:
                    log_to_file(self.log_file, f"‚ùå [Task {task.name_of_process} {task.source_page_name}] –û—à–∏–±–∫–∞ –≤ check_for_update: {e}")
                    continue

                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤ –ë–î, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å
                if task.changed:
                    try:
                        update_task_process_fields(session, task, self.log_file, table_name="SheetsInfo")
                    except Exception as e:
                        log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –ë–î: {e}")

            except Exception as e:
                log_to_file(self.log_file, f"‚ùå [Task {task.name_of_process} {task.source_page_name}] –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")

        # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç
        for task in self.tasks:
            log_to_file(
                self.log_file,
                f"‚ö™ [Task {task.name_of_process} {task.source_page_name}] –û—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ: {task.scanned} | "
                f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {task.proceed} | –ò–∑–º–µ–Ω–µ–Ω–æ: {task.changed} | –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {task.uploaded}"
            )

#############################################################################################
# –§–∞–∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
#############################################################################################

    def update_phase(self, session):
        log_section("üîº –§–∞–∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è", self.log_file)

        # --- –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á ---
        tasks_to_update = [t for t in self.tasks if t.values_json and t.changed and t.update_group not in {"update_mistakes_in_db", "feedback_status_update", "update_schedule_OT"}]
        mistakes_to_update = [t for t in self.tasks if t.values_json and t.changed and t.update_group == "update_mistakes_in_db"]
        feedback_to_update = [t for t in self.tasks if t.values_json and t.changed and t.update_group == "feedback_status_update"]
        schedule_OT_to_update = [t for t in self.tasks if t.values_json and t.changed and t.update_group == "update_schedule_OT"]

        log_to_file(self.log_file, f"üîº –ó–∞–¥–∞—á –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {len(tasks_to_update)}")
        for task in tasks_to_update:
            log_to_file(self.log_file, f"   ‚Ä¢ {task.name_of_process} ({task.update_group})\n")
        log_to_file(self.log_file, f"üîº –û—à–∏–±–æ–∫ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {len(mistakes_to_update)}")
        for task in mistakes_to_update:
            log_to_file(self.log_file, f"   ‚Ä¢ {task.name_of_process} ({task.update_group})\n")
        log_to_file(self.log_file, f"üîº –§–∏–¥–±–µ–∫–æ–≤ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {len(feedback_to_update)}")
        for task in feedback_to_update:
            log_to_file(self.log_file, f"   ‚Ä¢ {task.name_of_process} ({task.update_group})\n")
        log_to_file(self.log_file, f"üîº Schedule OT –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {len(schedule_OT_to_update)}")
        for task in schedule_OT_to_update:
            log_to_file(self.log_file, f"   ‚Ä¢ {task.name_of_process} ({task.update_group})\n")

        # # --- –û–±—ã—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ ---
        log_to_file(self.log_file, f"üîº –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á: {len(tasks_to_update)}")
        if tasks_to_update:
            try:
                self.import_tasks_to_update(tasks_to_update, session)
            except Exception as e:
                log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∑–∞–¥–∞—á: {e}")
            time.sleep(3)

        
        # --- –û—à–∏–±–∫–∏ (MistakeStorage) ---
        log_to_file(self.log_file, f"üîº –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫: {len(mistakes_to_update)}")
        if mistakes_to_update:
            try:
                self.import_mistakes_to_update(mistakes_to_update, session)
            except Exception as e:
                log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –æ—à–∏–±–æ–∫: {e}")
            time.sleep(3)

        # --- –§–∏–¥–±–µ–∫–∏ (FeedbackStorage) ---
        log_to_file(self.log_file, f"üîº –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ñ–∏–¥–±–µ–∫–æ–≤: {len(feedback_to_update)}")
        if feedback_to_update:
            try:
                self.import_feedbacks_to_update(feedback_to_update, self.service, session)
            except Exception as e:
                log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ñ–∏–¥–±–µ–∫–æ–≤: {e}")

        # --- Schedule GP ---
        log_to_file(self.log_file, f"üîº –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ GP: {len(feedback_to_update)}")
        if schedule_OT_to_update:
            try:
                self.import_schedule_OT_to_update(schedule_OT_to_update, session)
            except Exception as e:
                log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ schedule OT: {e}")
            time.sleep(3)

        # --- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–¥–∞—á–∞–º ---
        log_to_file(self.log_file, "üîº –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–¥–∞—á–∞–º:")
        for task in self.tasks:
            log_to_file(
                self.log_file,
                f"‚ö™ [Task {task.name_of_process} {task.source_page_name}] –û—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ: {task.scanned} | "
                f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {task.proceed} | –ò–∑–º–µ–Ω–µ–Ω–æ: {task.changed} | –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {task.uploaded}"
            )

        if not (tasks_to_update or mistakes_to_update or feedback_to_update):
            # log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è.")
            return

        log_section("üîº –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.", self.log_file)

##############################################################################################
# –ò–º–ø–æ—Ä—Ç –û–±—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á 
##############################################################################################

    def import_tasks_to_update(self, tasks_to_update, session):
        log_to_file(self.log_file, f"üîÑ –ù–∞—á–∞–ª–æ —Ñ–∞–∑—ã tasks_to_update. –í—Å–µ–≥–æ –∑–∞–¥–∞—á: {len(tasks_to_update)}")

        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –∑–∞–¥–∞—á –ø–æ update_group
        tasks_by_group = defaultdict(list)
        for task in tasks_to_update:
            tasks_by_group[task.update_group].append(task)

        for update_group, group_tasks in tasks_by_group.items():
            log_to_file(self.log_file, f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã: {update_group} ({len(group_tasks)} –∑–∞–¥–∞—á)")

            doc_id = group_tasks[0].target_doc_id
            batch_data = []

            for task in group_tasks:
                if not task.values_json:
                    # log_to_file(self.log_file, f"‚ö™ [Task {task.name_of_process} {task.source_page_name}] –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–ø—É—Å–∫.")
                    continue

                batch_data.append({
                    "range": f"{task.target_page_name}!{task.target_page_area}",
                    "values": task.values_json
                })

            if not batch_data:
                # log_to_file(self.log_file, f"‚ö™ –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è batchUpdate –≥—Ä—É–ø–ø—ã {update_group}.")
                continue
            # for data in batch_data:
            #     print(f"   ‚Ä¢ {data['range']} ({len(data['values'])} —Å—Ç—Ä–æ–∫)")

            # –ü–∞–∫–µ—Ç–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞
            success, error = batch_update(
                service=self.service,
                spreadsheet_id=doc_id,
                batch_data=batch_data,
                token_name=self.token_name,
                update_group=update_group,
                log_file=self.log_file
            )

            if success:
                log_to_file(self.log_file, f"‚úÖ –ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –¥–ª—è –≥—Ä—É–ø–ø—ã {update_group}")
                for task in group_tasks:
                    task.update_after_upload(success=True)
                    update_task_update_fields(
                        session=session,
                        task=task,
                        log_file=self.log_file,
                        table_name="SheetsInfo"
                    )
            else:
                log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞–∫–µ—Ç–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–µ: {error}")
                log_to_file(self.log_file, "üîÅ –ü–µ—Ä–µ—Ö–æ–¥ –∫ –æ–¥–∏–Ω–æ—á–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–µ –∑–∞–¥–∞—á")

                for task in group_tasks:
                    if not task.values_json:
                        continue

                    single_data = [{
                        "range": f"{task.target_page_name}!{task.target_page_area}",
                        "values": task.values_json
                    }]

                    single_success, single_error = batch_update(
                        service=self.service,
                        spreadsheet_id=doc_id,
                        batch_data=single_data,
                        token_name=self.token_name,
                        update_group=update_group,
                        log_file=self.log_file
                    )

                    if single_success:
                        log_to_file(self.log_file, f"‚úÖ [Task {task.name_of_process} {task.source_page_name}] –û–±–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ—à—Ç—É—á–Ω–æ.")
                    else:
                        log_to_file(self.log_file, f"‚ùå [Task {task.name_of_process} {task.source_page_name}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏: {single_error}")

                    task.update_after_upload(success=single_success)
                    update_task_update_fields(
                        session=session,
                        task=task,
                        log_file=self.log_file,
                        table_name="SheetsInfo"
                    )

###############################################################################################
# –ò–º–ø–æ—Ä—Ç –û—à–∏–±–æ–∫ –≤ –ë–î
###############################################################################################


    @staticmethod
    def parse_date(value):
        try:
            return datetime.strptime(value.strip(), "%d.%m.%Y").date()
        except Exception:
            return None

    @staticmethod
    def parse_time(value):
        try:
            return datetime.strptime(value.strip(), "%H:%M").time()
        except Exception:
            return None

    @staticmethod
    def parse_cancel(value):
        return 1 if str(value).strip().lower() == "cancel" else 0

    @staticmethod
    def determine_shift(hour: int) -> str:
        if 9 <= hour < 21:
            return "DAY"
        return "NIGHT"

    def import_mistakes_to_update(self, mistakes_to_update, session):

        try:
            for task in mistakes_to_update:
                sheet = task.raw_values_json
                page_name = task.source_page_name
                floor = get_floor_by_table_name(page_name, FLOORS)
                max_row_in_db = get_max_last_row(session, page_name)

                for row_index, row in enumerate(sheet[1:], start=2):  # –ø—Ä–æ–ø—É—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    if row_index <= max_row_in_db:
                        continue

                    if not row or len(row) < 8:
                        continue

                    try:
                        date = self.parse_date(row[0])
                        time_ = self.parse_time(row[1])
                        shift = self.determine_shift(time_.hour) if time_ else None

                        mistake = MistakeStorage(
                            related_month=task.related_month,
                            related_date=date,
                            related_shift=shift,
                            floor=floor,
                            table_name=page_name,
                            event_time=time_,
                            game_id=row[2],
                            mistake=row[3],
                            mistake_type=row[4],
                            is_cancel=self.parse_cancel(row[5]),
                            dealer_name=row[6],
                            sm_name=row[7],
                            last_row=row_index
                        )
                        session.add(mistake)

                    except Exception as row_err:
                        log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —Å—Ç—Ä–æ–∫–∏ {row_index} –∏–∑ {page_name}: {row_err}. –°—Ç—Ä–æ–∫–∞: {row}")
                        continue

            session.commit()
            log_to_file(self.log_file, "‚úÖ –í—Å–µ –æ—à–∏–±–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã.")

        except Exception as task_err:
            session.rollback()
            log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ mistakes_to_update: {task_err}")


################################################################################################
# –ò–º–ø–æ—Ä—Ç —Å—Ç–∞—Ç—É—Å–∞ —Ñ–∏–¥–±–µ–∫–æ–≤
################################################################################################

    # def update_gp_statuses_in_sheet(self, sheets_service, session):
    #     sheet_id = None
    #     target_range = None

    #     for task in self.tasks:
    #         if task.update_group == "feedback_status_update":
    #             sheet_id = task.target_doc_id
    #             target_range = f"{task.target_page_name}!{task.target_page_area}"
    #             break

    #     if not sheet_id or not target_range:
    #         # log_to_file(self.log_file, "‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –ª–∏—Å—Ç –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–æ–≤ GP.")
    #         return []

    #     try:
    #         records = session.query(
    #             FeedbackStorage.gp_name_surname,
    #             FeedbackStorage.reason,
    #             FeedbackStorage.forwarded_feedback
    #         ).filter(FeedbackStorage.gp_name_surname.isnot(None)).all()

    #         gp_status = {}
    #         for name, reason, forwarded in records:
    #             if reason and reason.strip():
    #                 gp_status[name] = "‚úÖ" if forwarded and forwarded.strip() else "‚ùå"
    #                 # log_to_file(self.log_file, f"‚úÖ –°—Ç–∞—Ç—É—Å GP: {name} - {gp_status[name]}")
    #             else:
    #                 gp_status[name] = "‚ùì"
    #                 # log_to_file(self.log_file, f"‚ùì –°—Ç–∞—Ç—É—Å GP: {name} - {gp_status[name]}")

    #         result = sheets_service.spreadsheets().values().get(
    #             spreadsheetId=sheet_id,
    #             range=self.doc_id_map.get(sheet_id, target_range),
    #         ).execute()

    #         sheet_names = []
    #         for row in result.get("values", []):
    #             if isinstance(row, list) and row and isinstance(row[0], str) and row[0].strip():
    #                 sheet_names.append(row[0].strip())

    #         # --- –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç—É—Å—ã –≤ FeedbackStatus ---
    #         for name in sheet_names:
    #             status = gp_status.get(name, "‚úÖ")  # –¢–µ–ø–µ—Ä—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚úÖ
    #             existing = session.query(FeedbackStatus).filter_by(name_surname=name).first()
    #             if existing:
    #                 existing.status = status
    #             else:
    #                 new_status = FeedbackStatus(name_surname=name, status=status)
    #                 session.add(new_status)
    #         session.commit()

    #         output = [[name, gp_status.get(name, "‚úÖ")] for name in sheet_names]  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚úÖ
    #         # for name in gp_status:
    #         #     print(f"GP: {name} - {gp_status[name]}")
    #         # log_to_file(self.log_file, f"‚úÖ –°—Ç–∞—Ç—É—Å—ã GP –æ–±–Ω–æ–≤–ª–µ–Ω—ã: {len(output)}")

    #         sheets_service.spreadsheets().values().update(
    #             spreadsheetId=sheet_id,
    #             range=target_range,
    #             valueInputOption="RAW",
    #             body={"values": output}
    #         ).execute()

    #         return output

    #     except Exception as e:
    #         session.rollback()
    #         log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ GP —Å—Ç–∞—Ç—É—Å–æ–≤: {e}")
    #         return []

    @staticmethod
    def safe_int(value):
        try:
            return int(value)
        except (ValueError, TypeError):
            return None

    def import_feedbacks_to_update(self, feedback_to_update, sheets_service, session):

        try:
            for task in feedback_to_update:
                sheet = task.raw_values_json
                page_name = task.target_page_name
                empty_row_streak = 0

                for row_index, row in enumerate(sheet[1:], start=2):
                    if not row or not str(row[0]).isdigit():
                        continue

                    feedback_id = int(row[0])
                    data = row[1:]

                    expected_len = 13
                    data = (data + [None] * expected_len)[:expected_len]

                    essential_fields = data[:8] + data[9:]
                    if all((str(f or '').strip() == '') for f in essential_fields):
                        empty_row_streak += 1
                    else:
                        empty_row_streak = 0

                    if empty_row_streak >= 15:
                        break

                    try:
                        parsed_date = self.parse_date(data[0])
                        shift = data[1]  # You can optionally standardize shift values here

                        existing = session.query(FeedbackStorage).filter_by(id=feedback_id).first()
                        if existing:
                            existing.related_month = task.related_month
                            for attr, val in zip([
                                "related_date", "related_shift", "floor", "game", "dealer_name",
                                "sm_name", "reason", "total", "proof",
                                "explanation_of_the_reason", "action_taken",
                                "forwarded_feedback", "comment_after_forwarding"
                            ], [parsed_date, shift] + data[2:]):
                                if attr == "total":
                                    val = self.safe_int(val)
                                setattr(existing, attr, val)
                        else:
                            new_feedback = FeedbackStorage(
                                id=feedback_id,
                                related_month=task.related_month,
                                related_date=parsed_date,
                                related_shift=shift,
                                floor=data[2],
                                game=data[3],
                                dealer_name=data[4],
                                sm_name=data[5],
                                reason=data[6],
                                total=self.safe_int(data[7]),
                                proof=data[8],
                                explanation_of_the_reason=data[9],
                                action_taken=data[10],
                                forwarded_feedback=data[11],
                                comment_after_forwarding=data[12]
                            )
                            session.add(new_feedback)

                    except Exception as row_err:
                        log_to_file(
                            self.log_file,
                            f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–æ–∫–∏ {row_index} –∏–∑ {page_name}: {row_err}. –°—Ç—Ä–æ–∫–∞: {row}"
                        )

            session.commit()
            log_to_file(self.log_file, "‚úÖ –í—Å–µ —Ñ–∏–¥–±–µ–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã.")

        except Exception as e:
            session.rollback()
            log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ —Ñ–∏–¥–±–µ–∫–æ–≤: {e}")

        finally:
            # self.update_gp_statuses_in_sheet(sheets_service, session)
            log_to_file(self.log_file, "üîÑ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ñ–∞–∑—ã feedback_status_update.")

###############################################################################################
# –ò–º–ø–æ—Ä—Ç Schedule OT
################################################################################################

    def import_schedule_OT_to_update(self, tasks, session):
        if not tasks:
            log_to_file(self.log_file, "‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á –≤ import_schedule_OT_to_update")
            return

        task = tasks[0]
        values = task.values_json
        related_month = task.related_month.replace(day=1)

        if not values or not isinstance(values, list):
            log_to_file(self.log_file, "‚ùå values_json –ø—É—Å—Ç –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω (–æ–∂–∏–¥–∞–µ—Ç—Å—è —Å–ø–∏—Å–æ–∫)")
            return

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏
            existing_records = session.query(ScheduleOT).filter_by(related_month=related_month).all()
            existing_lookup = {
                (rec.dealer_name.strip(), rec.date): rec
                for rec in existing_records if rec.dealer_name
            }

            new_entries = 0
            updated_entries = 0

            for row in values:
                if not row or not isinstance(row, list) or len(row) < 2:
                    continue

                dealer_name = row[0]
                if not dealer_name or not isinstance(dealer_name, str):
                    continue

                dealer_name = dealer_name.strip()

                for col_idx, shift in enumerate(row[1:], start=1):  # –î–µ–Ω—å –º–µ—Å—è—Ü–∞
                    shift = (shift or "").strip()
                    if shift in {"", "-", "/"}:
                        continue

                    try:
                        shift_date = related_month.replace(day=col_idx)
                    except ValueError:
                        continue  # –ù–∞–ø—Ä–∏–º–µ—Ä, 31 —Ñ–µ–≤—Ä–∞–ª—è

                    key = (dealer_name, shift_date)

                    if key in existing_lookup:
                        record = existing_lookup[key]
                        if record.shift_type != shift:
                            record.shift_type = shift
                            updated_entries += 1
                    else:
                        new_record = ScheduleOT(
                            date=shift_date,
                            dealer_name=dealer_name,
                            shift_type=shift,
                            related_month=related_month
                        )
                        session.add(new_record)
                        new_entries += 1

            session.commit()

            log_to_file(
                self.log_file,
                f"‚úÖ ScheduleOT: {related_month.strftime('%B %Y')} ‚Äî "
                f"{new_entries} –Ω–æ–≤—ã—Ö, {updated_entries} –æ–±–Ω–æ–≤–ª–µ–Ω–æ."
            )

        except Exception as e:
            session.rollback()
            log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ schedule OT: {e}")
