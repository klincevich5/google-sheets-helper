# scanners/sheetsinfo_scanner.py

import time
from datetime import datetime, timedelta
from collections import defaultdict

from tg_bot.utils.settings_access import is_scanner_enabled
from core.data import load_sheetsinfo_tasks
from utils.logger import log_to_file, log_separator, log_section
from utils.floor_resolver import get_floor_by_table_name
from database.session import SessionLocal

from database.db_models import MistakeStorage, FeedbackStorage,  ScheduleOT, DealerMonthlyStatus
from utils.db_orm import get_max_last_row

from core.config import (
    SHEETSINFO_LOG,
    SHEETINFO_INTERVAL,
    FLOORS
)
from utils.db_orm import (
    update_task_scan_fields,
    update_task_process_fields,
    update_task_update_fields
)
from utils.utils import (
    load_credentials,
    check_sheet_exists,
    batch_get,
    batch_update,
)

class SheetsInfoScanner:
    def __init__(self, token_map, doc_id_map):
        self.token_map = token_map
        self.doc_id_map = doc_id_map
        self.log_file = SHEETSINFO_LOG
        self.tasks = []

    def run(self):
        while True:
            try:
                if not is_scanner_enabled("sheets_scanner"):
                    time.sleep(10)
                    continue

                log_section("‚ñ∂Ô∏è SheetsInfo –ê–∫—Ç–∏–≤–µ–Ω. –ù–æ–≤—ã–π —Ü–∏–∫–ª —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", self.log_file)

                token_name = list(self.token_map.keys())[0]
                token_path = self.token_map[token_name]
                self.token_name = token_name

                with SessionLocal() as session:
                    self.service = load_credentials(token_path, self.log_file)
                    log_to_file(self.log_file, f"üîê –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–∫–µ–Ω: {self.token_name}")

                    for phase_name, method in [
                        ("–∑–∞–≥—Ä—É–∑–∫–∏ –∑–∞–¥–∞—á", lambda: self.load_tasks(session)),
                        ("—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", lambda: self.scan_phase(session)),
                        ("–æ–±—Ä–∞–±–æ—Ç–∫–∏", lambda: self.process_phase(session)),
                        ("–æ–±–Ω–æ–≤–ª–µ–Ω–∏—è", lambda: self.update_phase(session))
                    ]:
                        try:
                            method()
                        except Exception as e:
                            log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –Ω–∞ —ç—Ç–∞–ø–µ {phase_name}: {e}")
                            raise

            except Exception as e:
                log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ SheetsInfo: {e}")
                time.sleep(10)

            time.sleep(SHEETINFO_INTERVAL)

#############################################################################################
# –∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–¥–∞—á –∏–∑ –ë–î
#############################################################################################

    def load_tasks(self, session):
        log_section("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–¥–∞—á –∏–∑ SheetsInfo", self.log_file)

        self.tasks = load_sheetsinfo_tasks(session, self.log_file)

        if not self.tasks:
            # log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.")
            self.tasks = []
            return

        # log_to_file(self.log_file, f"üîÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–¥–∞—á: {len(self.tasks)}")

        skipped = 0
        for task in self.tasks:
            if not task.assign_doc_ids(self.doc_id_map):
                skipped += 1

        # if skipped:
        #     log_to_file(self.log_file, f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–æ –∑–∞–¥–∞—á –±–µ–∑ doc_id: {skipped}")

#############################################################################################
# –§–∞–∑–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
#############################################################################################

    def scan_phase(self, session):
        # log_section("üîç –§–∞–∑–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", self.log_file)

        if not self.tasks:
            # log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.")
            return

        ready_tasks = [task for task in self.tasks if task.is_ready_to_scan()]
        if not ready_tasks:
            # log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á, –≥–æ—Ç–æ–≤—ã—Ö –∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é.")
            return

        # log_to_file(self.log_file, f"üîé –ù–∞–π–¥–µ–Ω–æ {len(ready_tasks)} –∑–∞–¥–∞—á, –≥–æ—Ç–æ–≤—ã—Ö –∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é:")

        scan_groups = defaultdict(list)
        for task in ready_tasks:
            if not task.assign_doc_ids(self.doc_id_map):
                # log_to_file(self.log_file, f"‚ö†Ô∏è [Task {task.name_of_process} {task.source_page_name}] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å doc_id. –ü—Ä–æ–ø—É—Å–∫.")
                continue
            scan_groups[task.scan_group].append(task)

        for scan_group, group_tasks in scan_groups.items():
            # log_separator(self.log_file)
            # log_to_file(self.log_file, f"üìò –û–±—Ä–∞–±–æ—Ç–∫–∞ scan_group: {scan_group} ({len(group_tasks)} –∑–∞–¥–∞—á)")

            if not group_tasks:
                # log_to_file(self.log_file, "‚ö™ –í –≥—Ä—É–ø–ø–µ –Ω–µ—Ç –∑–∞–¥–∞—á.")
                continue

            doc_id = group_tasks[0].source_doc_id
            unique_sheet_names = set(task.source_page_name for task in group_tasks)
            # log_to_file(self.log_file, f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è –ª–∏—Å—Ç–æ–≤: {unique_sheet_names}")

            exists_map = {
                sheet_name: check_sheet_exists(self.service, doc_id, sheet_name, self.log_file, self.token_name)
                for sheet_name in unique_sheet_names
            }

            # for sheet_name, exists in exists_map.items():
            #     log_to_file(self.log_file, f"{'‚úÖ' if exists else '‚ö†Ô∏è'} –õ–∏—Å—Ç '{sheet_name}' {'—Å—É—â–µ—Å—Ç–≤—É–µ—Ç' if exists else '–Ω–µ –Ω–∞–π–¥–µ–Ω'}.")

            valid_tasks = []
            for task in group_tasks:
                sheet_name = task.source_page_name
                if exists_map.get(sheet_name):
                    # log_to_file(self.log_file, f"‚û°Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º '{sheet_name}' –¥–ª—è –∑–∞–¥–∞—á–∏ {task.name_of_process}.")
                    valid_tasks.append(task)
                else:
                    # log_to_file(self.log_file, f"‚õî –ü—Ä–æ–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ {task.name_of_process}: –ª–∏—Å—Ç '{sheet_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω.")
                    task.update_after_scan(success=False)
                    update_task_scan_fields(session, task, self.log_file, table_name="SheetsInfo")

            if not valid_tasks:
                # log_to_file(self.log_file, f"‚ö™ –í—Å–µ –∑–∞–¥–∞—á–∏ –≥—Ä—É–ø–ø—ã {scan_group} –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã. –ü—Ä–æ–ø—É—Å–∫ batchGet.")
                continue

            range_to_tasks = defaultdict(list)
            for task in valid_tasks:
                range_str = f"{task.source_page_name}!{task.source_page_area}"
                range_to_tasks[range_str].append(task)

            ranges = list(range_to_tasks.keys())

            log_to_file(self.log_file, "")
            log_to_file(self.log_file, f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ batchGet –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç {task.source_table_type} —Å {len(ranges)} —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏:")
            
            for r in ranges:
                log_to_file(self.log_file, f"   ‚Ä¢ {r}")

            response_data = batch_get(
                self.service,
                doc_id,
                ranges,
                scan_group,
                self.log_file,
                self.token_name
            )
            if not response_data:
                # log_to_file(self.log_file, "‚ùå –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç batchGet. –í—Å–µ –∑–∞–¥–∞—á–∏ –±—É–¥—É—Ç –æ—Ç–º–µ—á–µ–Ω—ã –∫–∞–∫ –Ω–µ—É–¥–∞—á–Ω—ã–µ.")
                for task in valid_tasks:
                    task.update_after_scan(success=False)
                    update_task_scan_fields(session, task, self.log_file, table_name="SheetsInfo")
                continue

            normalized_response = {}
            for k, v in response_data.items():
                clean_key = k.replace("'", "")
                if "!" in clean_key:
                    sheet_name, cells_range = clean_key.split("!", 1)
                    normalized_response[(sheet_name.strip(), cells_range.strip())] = v

            # log_to_file(self.log_file, "")
            # log_to_file(self.log_file, f"üì• –ü–æ–ª—É—á–µ–Ω—ã –¥–∏–∞–ø–∞–∑–æ–Ω—ã: {list(normalized_response.keys())}")

            for task in valid_tasks:
                expected_sheet = task.source_page_name.strip()
                expected_area_start = task.source_page_area.split(":")[0].strip()
                matched_values = None

                for (sheet_name, cells_range), values in normalized_response.items():
                    if sheet_name == expected_sheet and cells_range.startswith(expected_area_start):
                        matched_values = values
                        break

                if matched_values:
                    task.raw_values_json = matched_values
                    task.update_after_scan(success=True)
                    update_task_scan_fields(session, task, self.log_file, table_name="SheetsInfo")
                    log_to_file(self.log_file, f"‚úÖ [Task {task.name_of_process} {task.source_page_name}] –ù–∞–π–¥–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω {sheet_name}!{cells_range}, —Å—Ç—Ä–æ–∫: {len(matched_values)}")
                else:
                    task.update_after_scan(success=False)
                    update_task_scan_fields(session, task, self.log_file, table_name="SheetsInfo")
                    log_to_file(self.log_file, f"‚ö†Ô∏è [Task {task.name_of_process} {task.source_page_name}] –î–∏–∞–ø–∞–∑–æ–Ω {expected_sheet}!{task.source_page_area} –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç.")

        for task in self.tasks:
            log_to_file(
                self.log_file,
                f"‚ö™ [Task {task.name_of_process} {task.source_page_name}] –û—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ: {task.scanned} | "
                f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {task.proceed} | –ò–∑–º–µ–Ω–µ–Ω–æ: {task.changed} | –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {task.uploaded}"
            )
        log_to_file(self.log_file, "üîç –§–∞–∑–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

#############################################################################################
# –§–∞–∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
#############################################################################################

    def process_phase(self, session):
        log_section("üõ†Ô∏è –§–∞–∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏", self.log_file)

        if not self.tasks:
            log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
            return

        for task in self.tasks:
            if task.scanned == 0:
                continue

            try:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å—ã—Ä—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                try:
                    task.process_raw_value()
                except Exception as e:
                    log_to_file(self.log_file, f"‚ùå [Task {task.name_of_process} {task.source_page_name}] –û—à–∏–±–∫–∞ –≤ process_raw_value: {e}")
                    continue

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π
                try:
                    task.check_for_update()
                except Exception as e:
                    log_to_file(self.log_file, f"‚ùå [Task {task.name_of_process} {task.source_page_name}] –û—à–∏–±–∫–∞ –≤ check_for_update: {e}")
                    continue

                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤ –ë–î, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å
                if task.changed:
                    try:
                        update_task_process_fields(session, task, self.log_file, table_name="SheetsInfo")
                    except Exception as e:
                        log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤ –ë–î: {e}")

            except Exception as e:
                log_to_file(self.log_file, f"‚ùå [Task {task.name_of_process} {task.source_page_name}] –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}")

        # –ò—Ç–æ–≥–æ–≤—ã–π –æ—Ç—á—ë—Ç
        for task in self.tasks:
            log_to_file(
                self.log_file,
                f"‚ö™ [Task {task.name_of_process} {task.source_page_name}] –û—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ: {task.scanned} | "
                f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {task.proceed} | –ò–∑–º–µ–Ω–µ–Ω–æ: {task.changed} | –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {task.uploaded}"
            )

#############################################################################################
# –§–∞–∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
#############################################################################################
    def log_tasks_by_type(self, tasks, label):
        log_to_file(self.log_file, f"üîº {label}: {len(tasks)}")
        for task in tasks:
            log_to_file(self.log_file, f"   ‚Ä¢ {task.name_of_process} ({task.update_group})")

    def update_phase(self, session):
        log_section("üîº –§–∞–∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è", self.log_file)

        try:
            # --- –ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á ---
            tasks_to_update = [t for t in self.tasks if t.values_json and t.changed and t.update_group not in {
                "update_mistakes_in_db", "feedback_status_update", "update_schedule_OT", "update_qa_list_db"}]
            mistakes_to_update = [t for t in self.tasks if t.values_json and t.changed and t.update_group == "update_mistakes_in_db"]
            feedback_to_update = [t for t in self.tasks if t.values_json and t.changed and t.update_group == "feedback_status_update"]
            schedule_OT_to_update = [t for t in self.tasks if t.values_json and t.changed and t.update_group == "update_schedule_OT"]
            qa_list_update = [t for t in self.tasks if t.values_json and t.changed and t.update_group == "update_qa_list_db"]

            # --- –õ–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π ---
            self.log_tasks_by_type(tasks_to_update, "–ó–∞–¥–∞—á –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")
            self.log_tasks_by_type(mistakes_to_update, "–û—à–∏–±–æ–∫ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")
            self.log_tasks_by_type(feedback_to_update, "–§–∏–¥–±–µ–∫–æ–≤ –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")
            self.log_tasks_by_type(schedule_OT_to_update, "Schedule OT –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")
            self.log_tasks_by_type(qa_list_update, "QA List –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")

            # --- –û–±—ã—á–Ω—ã–µ –∑–∞–¥–∞—á–∏ ---
            log_to_file(self.log_file, f"üîº –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á: {len(tasks_to_update)}")
            if tasks_to_update:
                try:
                    self.import_tasks_to_update(tasks_to_update, session)
                except Exception as e:
                    log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∑–∞–¥–∞—á: {e}")
                time.sleep(3)

            # --- –û—à–∏–±–∫–∏ ---
            log_to_file(self.log_file, f"üîº –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫: {len(mistakes_to_update)}")
            if mistakes_to_update:
                try:
                    self.import_mistakes_to_update(mistakes_to_update, session)
                except Exception as e:
                    log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –æ—à–∏–±–æ–∫: {e}")
                time.sleep(3)

            # --- –§–∏–¥–±–µ–∫–∏ ---
            log_to_file(self.log_file, f"üîº –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Ñ–∏–¥–±–µ–∫–æ–≤: {len(feedback_to_update)}")
            if feedback_to_update:
                try:
                    self.import_feedbacks_to_update(feedback_to_update, self.service, session)
                except Exception as e:
                    log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Ñ–∏–¥–±–µ–∫–æ–≤: {e}")

            # --- Schedule OT ---
            log_to_file(self.log_file, f"üîº –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤ OT: {len(schedule_OT_to_update)}")
            if schedule_OT_to_update:
                try:
                    self.import_schedule_OT_to_update(schedule_OT_to_update, session)
                except Exception as e:
                    log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ schedule OT: {e}")
                time.sleep(3)

            # --- QA List ---
            log_to_file(self.log_file, f"üîº –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ QA List: {len(qa_list_update)}")
            if qa_list_update:
                try:
                    self.import_qa_list_to_update(qa_list_update, session)
                except Exception as e:
                    log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ QA List: {e}")
                time.sleep(3)

            # --- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–¥–∞—á–∞–º ---
            log_to_file(self.log_file, "üîº –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∑–∞–¥–∞—á–∞–º:")
            for task in self.tasks:
                log_to_file(
                    self.log_file,
                    f"‚ö™ [Task {task.name_of_process} {task.source_page_name}] "
                    f"–û—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ: {task.scanned} | –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {task.proceed} | "
                    f"–ò–∑–º–µ–Ω–µ–Ω–æ: {task.changed} | –ó–∞–≥—Ä—É–∂–µ–Ω–æ: {task.uploaded}"
                )

            if not (tasks_to_update or mistakes_to_update or feedback_to_update or schedule_OT_to_update or qa_list_update):
                return

        finally:
            log_section("üîº –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.", self.log_file)

##############################################################################################
# –ò–º–ø–æ—Ä—Ç –û–±—ã—á–Ω—ã—Ö –∑–∞–¥–∞—á 
##############################################################################################


    def import_tasks_to_update(self, tasks_to_update, session):
        log_to_file(self.log_file, f"üîÑ –ù–∞—á–∞–ª–æ —Ñ–∞–∑—ã tasks_to_update. –í—Å–µ–≥–æ –∑–∞–¥–∞—á: {len(tasks_to_update)}")

        tasks_by_group = defaultdict(list)
        for task in tasks_to_update:
            tasks_by_group[task.update_group].append(task)

        for update_group, group_tasks in tasks_by_group.items():
            log_to_file(self.log_file, f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã: {update_group} ({len(group_tasks)} –∑–∞–¥–∞—á)")

            doc_id = group_tasks[0].target_doc_id
            batch_data = self._build_batch_data(group_tasks)

            if not batch_data:
                log_to_file(self.log_file, f"‚ö†Ô∏è –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è batchUpdate –≥—Ä—É–ø–ø—ã {update_group}. –ü—Ä–æ–ø—É—Å–∫.")
                continue

            success, error = batch_update(
                service=self.service,
                spreadsheet_id=doc_id,
                batch_data=batch_data,
                token_name=self.token_name,
                update_group=update_group,
                log_file=self.log_file
            )

            if success:
                log_to_file(self.log_file, f"‚úÖ –ü–∞–∫–µ—Ç–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –¥–ª—è –≥—Ä—É–ø–ø—ã {update_group}")
                self._mark_tasks_uploaded(group_tasks, session)
            else:
                log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞–∫–µ—Ç–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–µ: {error}")
                self._fallback_single_upload(group_tasks, doc_id, update_group, session)

    def _build_batch_data(self, tasks):
        batch_data = []
        for task in tasks:
            if not task.values_json:
                continue
            batch_data.append({
                "range": f"{task.target_page_name}!{task.target_page_area}",
                "values": task.values_json
            })
        return batch_data

    def _mark_tasks_uploaded(self, tasks, session):
        for task in tasks:
            task.update_after_upload(success=True)
            update_task_update_fields(
                session=session,
                task=task,
                log_file=self.log_file,
                table_name="SheetsInfo"
            )

    def _fallback_single_upload(self, tasks, doc_id, update_group, session):
        for task in tasks:
            if not task.values_json:
                continue

            single_data = [{
                "range": f"{task.target_page_name}!{task.target_page_area}",
                "values": task.values_json
            }]

            success, error = batch_update(
                service=self.service,
                spreadsheet_id=doc_id,
                batch_data=single_data,
                token_name=self.token_name,
                update_group=update_group,
                log_file=self.log_file
            )

            if success:
                log_to_file(self.log_file, f"‚úÖ [Task {task.name_of_process} {task.source_page_name}] –û–±–Ω–æ–≤–ª–µ–Ω–∞ –ø–æ—à—Ç—É—á–Ω–æ.")
            else:
                log_to_file(self.log_file, f"‚ùå [Task {task.name_of_process} {task.source_page_name}] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏: {error}")

            task.update_after_upload(success=success)
            update_task_update_fields(
                session=session,
                task=task,
                log_file=self.log_file,
                table_name="SheetsInfo"
            )

###############################################################################################
# –ò–º–ø–æ—Ä—Ç –û—à–∏–±–æ–∫ –≤ –ë–î
###############################################################################################

    def import_mistakes_to_update(self, mistakes_to_update, session):
        success_count = 0
        error_count = 0

        try:
            for task in mistakes_to_update:
                sheet = task.raw_values_json
                if not sheet or not isinstance(sheet, list):
                    log_to_file(self.log_file, f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π sheet –≤ –∑–∞–¥–∞—á–µ: {task.name_of_process}")
                    continue

                page_name = task.source_page_name
                floor = get_floor_by_table_name(page_name, FLOORS)
                max_row_in_db = get_max_last_row(session, page_name)

                for row_index, row in enumerate(sheet[1:], start=2):  # –ø—Ä–æ–ø—É—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    if row_index <= max_row_in_db or not row or len(row) < 8:
                        continue

                    try:
                        mistake = self._parse_mistake_row(task, row, row_index, floor, page_name)
                        if mistake:
                            session.add(mistake)
                            success_count += 1
                    except Exception as row_err:
                        log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —Å—Ç—Ä–æ–∫–∏ {row_index} –∏–∑ {page_name}: {row_err}. –°—Ç—Ä–æ–∫–∞: {row}")
                        error_count += 1

            session.commit()
            log_to_file(self.log_file, f"‚úÖ –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –æ—à–∏–±–æ–∫: {success_count}, –æ—à–∏–±–æ–∫: {error_count}")

        except Exception as task_err:
            session.rollback()
            log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ mistakes_to_update: {task_err}")

    def _parse_mistake_row(self, task, row, row_index, floor, page_name):
        date = self.parse_date(row[0])
        time_ = self.parse_time(row[1])
        shift = self.determine_shift(time_.hour) if time_ else None

        return MistakeStorage(
            related_month=task.related_month,
            related_date=date,
            related_shift=shift,
            floor=floor,
            table_name=page_name,
            event_time=time_,
            game_id=row[2],
            mistake=row[3],
            mistake_type=row[4],
            is_cancel=self.parse_cancel(row[5]),
            dealer_name=row[6],
            sm_name=row[7],
            last_row=row_index
        )


################################################################################################
# –ò–º–ø–æ—Ä—Ç —Å—Ç–∞—Ç—É—Å–∞ —Ñ–∏–¥–±–µ–∫–æ–≤
################################################################################################

    @staticmethod
    def parse_date(value):
        try:
            return datetime.strptime(value.strip(), "%d.%m.%Y").date()
        except Exception:
            return None

    def import_feedbacks_to_update(self, feedback_to_update, sheets_service, session):
        success_count = 0
        error_count = 0

        try:
            for task in feedback_to_update:
                sheet = task.raw_values_json
                if not sheet or not isinstance(sheet, list):
                    log_to_file(self.log_file, f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π sheet –≤ –∑–∞–¥–∞—á–µ: {task.name_of_process}")
                    continue

                page_name = task.target_page_name
                empty_row_streak = 0

                for row_index, row in enumerate(sheet[1:], start=2):  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                    if not row or not str(row[0]).isdigit():
                        continue

                    feedback_id = int(row[0])
                    try:
                        parsed = self._parse_feedback_row(row[1:], task)
                        if parsed is None:
                            empty_row_streak += 1
                            if empty_row_streak >= 15:
                                break
                            continue
                        else:
                            empty_row_streak = 0

                        existing = session.query(FeedbackStorage).filter_by(id=feedback_id).first()
                        if existing:
                            for attr, val in parsed.items():
                                setattr(existing, attr, val)
                            existing.related_month = task.related_month
                        else:
                            session.add(FeedbackStorage(id=feedback_id, related_month=task.related_month, **parsed))

                        success_count += 1

                    except Exception as row_err:
                        error_count += 1
                        log_to_file(
                            self.log_file,
                            f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–æ–∫–∏ {row_index} –∏–∑ {page_name}: {row_err}. –°—Ç—Ä–æ–∫–∞: {row}"
                        )

            session.commit()
            log_to_file(self.log_file, f"‚úÖ –§–∏–¥–±–µ–∫–∏ —É—Å–ø–µ—à–Ω–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã: {success_count}, –æ—à–∏–±–æ–∫: {error_count}")

            # --- –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ DealerMonthlyStatus ---
            log_to_file(self.log_file, "üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ DealerMonthlyStatus –ø–æ —Ñ–∏–¥–±–µ–∫–∞–º...")

            dealers = session.query(DealerMonthlyStatus).filter_by(related_month=task.related_month).all()
            output_data = []

            for dealer in dealers:
                feedbacks = session.query(FeedbackStorage).filter_by(
                    dealer_name=dealer.dealer_name,
                    related_month=dealer.related_month
                ).all()

                if not feedbacks:
                    dealer.feedback_status = False
                    output_data.append([dealer.dealer_name, "‚ùå"])
                    continue

                if any(f.forwarded_feedback is None for f in feedbacks):
                    dealer.feedback_status = False
                    output_data.append([dealer.dealer_name, "‚ùå"])
                else:
                    dealer.feedback_status = True
                    output_data.append([dealer.dealer_name, "‚úÖ"])

            session.commit()
            log_to_file(self.log_file, f"‚úÖ –û–±–Ω–æ–≤–ª–µ–Ω–æ DealerMonthlyStatus: {len(output_data)} –∑–∞–ø–∏—Å–µ–π")

            # --- –í—ã–≥—Ä—É–∑–∫–∞ –≤ Google Sheets ---
            try:
                sheets_service.write_values(
                    task.target_page_name,
                    task.target_page_area,
                    output_data
                )
                log_to_file(self.log_file, f"üì§ –í—ã–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç—É—Å–æ–≤ –≤ Google Sheet –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {task.target_page_name} ({task.target_page_area})")
            except Exception as gs_err:
                log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–≥—Ä—É–∑–∫–µ –≤ Google Sheet: {gs_err}")

        except Exception as e:
            session.rollback()
            log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ —Ñ–∏–¥–±–µ–∫–æ–≤: {e}")

        finally:
            log_to_file(self.log_file, "üîÑ –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ñ–∞–∑—ã feedback_status_update.")

    def _parse_feedback_row(self, data_row, task):
        expected_len = 13
        data = (data_row + [None] * expected_len)[:expected_len]

        essential_fields = data[:8] + data[9:]
        if all((str(f or '').strip() == '') for f in essential_fields):
            return None

        parsed_date = self.parse_date(data[0])
        shift = data[1]  # –º–æ–∂–Ω–æ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å

        return {
            "related_date": parsed_date,
            "related_shift": shift,
            "floor": data[2],
            "game": data[3],
            "dealer_name": data[4],
            "sm_name": data[5],
            "reason": data[6],
            "total": self.safe_int(data[7]),
            "proof": data[8],
            "explanation_of_the_reason": data[9],
            "action_taken": data[10],
            "forwarded_feedback": data[11],
            "comment_after_forwarding": data[12]
        }

###############################################################################################
# –ò–º–ø–æ—Ä—Ç Schedule OT
################################################################################################

    def import_schedule_OT_to_update(self, tasks, session):
        if not tasks:
            log_to_file(self.log_file, "‚ö†Ô∏è –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –∑–∞–¥–∞—á –≤ import_schedule_OT_to_update")
            return

        try:
            total_new = 0
            total_updated = 0

            for task in tasks:
                values = task.values_json
                if not values or not isinstance(values, list):
                    log_to_file(self.log_file, f"‚ùå values_json –ø—É—Å—Ç –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω –≤ –∑–∞–¥–∞—á–µ {task.name_of_process}")
                    continue

                related_month = task.related_month.replace(day=1)
                existing_records = session.query(ScheduleOT).filter_by(related_month=related_month).all()
                existing_lookup = {
                    (rec.dealer_name.strip(), rec.date): rec
                    for rec in existing_records if rec.dealer_name
                }

                new_entries = 0
                updated_entries = 0

                for row in values:
                    if not row or not isinstance(row, list) or len(row) < 2:
                        continue

                    dealer_name = row[0]
                    if not dealer_name or not isinstance(dealer_name, str):
                        continue

                    dealer_name = dealer_name.strip()

                    for col_idx, shift in enumerate(row[1:], start=1):
                        shift = (shift or "").strip()
                        if shift in {"", "-", "/"}:
                            continue

                        try:
                            shift_date = related_month.replace(day=col_idx)
                        except ValueError:
                            continue

                        key = (dealer_name, shift_date)

                        if key in existing_lookup:
                            record = existing_lookup[key]
                            if record.shift_type != shift:
                                record.shift_type = shift
                                updated_entries += 1
                        else:
                            session.add(ScheduleOT(
                                date=shift_date,
                                dealer_name=dealer_name,
                                shift_type=shift,
                                related_month=related_month
                            ))
                            new_entries += 1

                log_to_file(self.log_file, f"üìÖ ScheduleOT: {task.name_of_process} ‚Äî –Ω–æ–≤—ã—Ö: {new_entries}, –æ–±–Ω–æ–≤–ª–µ–Ω–æ: {updated_entries}")
                total_new += new_entries
                total_updated += updated_entries

            session.commit()
            log_to_file(self.log_file, f"‚úÖ ScheduleOT –∏—Ç–æ–≥–æ ‚Äî –Ω–æ–≤—ã—Ö: {total_new}, –æ–±–Ω–æ–≤–ª–µ–Ω–æ: {total_updated}")

        except Exception as e:
            session.rollback()
            log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–º–ø–æ—Ä—Ç–µ schedule OT: {e}")


###############################################################################################
# –ò–º–ø–æ—Ä—Ç QA List –≤ –ë–î
###############################################################################################

    def import_qa_list_to_update(self, qa_list_update, session):
        success_count = 0
        error_count = 0

        try:
            for task in qa_list_update:
                sheet = task.raw_values_json
                if not sheet or not isinstance(sheet, list):
                    log_to_file(self.log_file, f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π sheet –≤ –∑–∞–¥–∞—á–µ: {task.name_of_process}")
                    continue

                page_name = task.source_page_name
                for row_index, row in enumerate(sheet[1:], start=2):  # –ø—Ä–æ–ø—É—Å–∫ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                    if not row or len(row) < 18:
                        log_to_file(self.log_file, f"‚ö†Ô∏è –ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–µ–ø–æ–ª–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ {row_index} –≤ {page_name}")
                        continue

                    try:
                        qa_item = self._parse_qa_list_row(task, row, row_index, page_name)
                        if qa_item:
                            session.add(qa_item)
                            success_count += 1
                    except Exception as row_err:
                        log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ —Å—Ç—Ä–æ–∫–∏ {row_index} –∏–∑ {page_name}: {row_err}. –°—Ç—Ä–æ–∫–∞: {row}")
                        error_count += 1

            session.commit()
            log_to_file(self.log_file, f"‚úÖ –ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ QA –∑–∞–ø–∏—Å–µ–π: {success_count}, –æ—à–∏–±–æ–∫: {error_count}")

        except Exception as task_err:
            session.rollback()
            log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ qa_list_update: {task_err}")


    def _parse_qa_list_row(self, task, row, row_index, page_name):
        return QaList(
            dealer_name=row[0],
            VIP=row[1],
            GENERIC=row[2],
            LEGENDZ=row[3],
            GSBJ=row[4],
            TURKISH=row[5],
            TRISTAR=row[6],
            TritonRL=row[7],
            QA_comment=row[8],
            Male=row[9],
            BJ=row[10],
            BC=row[11],
            RL=row[12],
            DT=row[13],
            HSB=row[14],
            swBJ=row[15],
            swBC=row[16],
            swRL=row[17],
            SH=row[18],
            gsDT=row[19]
        )
