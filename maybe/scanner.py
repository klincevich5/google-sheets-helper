import time
import json
import hashlib
from datetime import datetime

from config import WARSAW_TZ, SHEETS_LOG_FILE, SCAN_INTERVAL_SECONDS
from db import get_ready_tasks, update_task_scan, update_task_import
from logger import log_to_file
from sheets import load_sheet_api, batch_get_ranges, batch_update_ranges
from processors.registry import get_processor


def run_scanner():
    log_to_file(SHEETS_LOG_FILE, "üü¢ –°–∫–∞–Ω–µ—Ä –∑–∞–ø—É—â–µ–Ω.")

    while True:
        # 1. –ü–æ–ª—É—á–∞–µ–º –∑–∞–¥–∞—á–∏ –Ω–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
        tasks = get_ready_tasks()
        if not tasks:
            log_to_file(SHEETS_LOG_FILE, "‚è≥ –ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.")
            time.sleep(SCAN_INTERVAL_SECONDS)
            continue

        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–ª–∏–µ–Ω—Ç API
        sheet = load_sheet_api()
        if not sheet:
            log_to_file(SHEETS_LOG_FILE, "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Google Sheets.")
            time.sleep(SCAN_INTERVAL_SECONDS)
            continue

        log_to_file(SHEETS_LOG_FILE, f"üìÑ –ó–∞–¥–∞—á –Ω–∞ —Å–∫–∞–Ω: {len(tasks)}")

        for task in tasks:
            try:
                doc_id = task["source_doc_id"]
                sheet_range = f"'{task['source_page_name']}'!{task['source_page_area']}"
                values = batch_get_ranges(sheet, doc_id, [sheet_range])[0]
                now_str = datetime.now(WARSAW_TZ).isoformat()

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ process_data_method
                processor = get_processor(task["process_data_method"])
                processed = processor(values)
                values_json = json.dumps(processed, ensure_ascii=False)
                values_hash = hashlib.md5(values_json.encode("utf-8")).hexdigest()

                # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º —Ö—ç—à–µ–º
                if values_hash != task.get("hash"):
                    log_to_file(SHEETS_LOG_FILE, f"üîÅ ID={task['id']} | –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∏–∑–º–µ–Ω–µ–Ω–∏—è.")
                    task["values_json"] = values_json
                    task["hash"] = values_hash
                    task["needs_update"] = True
                else:
                    log_to_file(SHEETS_LOG_FILE, f"‚úÖ ID={task['id']} | –ë–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.")
                    task["needs_update"] = False

                task["last_scan"] = now_str
                task["scan_quantity"] = task.get("scan_quantity", 0) + 1
                update_task_scan(task)

            except Exception as e:
                log_to_file(SHEETS_LOG_FILE, f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ ID={task['id']}: {str(e)}")

        # –ò–º–ø–æ—Ä—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π
        tasks_to_import = [t for t in tasks if t.get("needs_update")]
        if tasks_to_import:
            run_import(sheet, tasks_to_import)

        time.sleep(SCAN_INTERVAL_SECONDS)


def run_import(sheet, tasks):
    log_to_file(SHEETS_LOG_FILE, f"‚¨áÔ∏è –ò–º–ø–æ—Ä—Ç {len(tasks)} –∏–∑–º–µ–Ω—ë–Ω–Ω—ã—Ö –∑–∞–¥–∞—á...")
    for task in tasks:
        try:
            values = json.loads(task["values_json"])
            batch_update_ranges(sheet, task["target_doc_id"], [{
                "range": f"'{task['target_page_name']}'!{task['target_page_area']}",
                "values": values
            }])
            task["last_update"] = datetime.now(WARSAW_TZ).isoformat()
            task["update_quantity"] = task.get("update_quantity", 0) + 1
            update_task_import(task)
            log_to_file(SHEETS_LOG_FILE, f"‚úÖ ID={task['id']} | –ò–º–ø–æ—Ä—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω.")
        except Exception as e:
            log_to_file(SHEETS_LOG_FILE, f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ ID={task['id']}: {str(e)}")
