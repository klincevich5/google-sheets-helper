# rotationsinfo_scanner.py

import os
import threading
import time
import json
from collections import defaultdict
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from googleapiclient.errors import HttpError

from config import ROTATIONSINFO_LOG
from data import load_rotationsinfo_tasks
from logger import log_to_file, log_separator, log_section


class RotationsInfoScanner:
    def __init__(self, conn, service, doc_id_map):
        self.conn = conn
        self.service = service
        self.doc_id_map = doc_id_map
        self.tasks = []
        self.log_file = ROTATIONSINFO_LOG
        self.keep_running = True

    def run(self):
        def heartbeat():
            while self.keep_running:
                log_to_file(self.log_file, "‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ü–∏–∫–ª–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è...")
                log_to_file(self.log_file, "=" * 100)
                time.sleep(10)

        threading.Thread(target=heartbeat, daemon=True).start()

        while True:
            try:
                log_section("üîÑ –ù–æ–≤—ã–π —Ü–∏–∫–ª —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è RotationsInfo", self.log_file)

                self.check_and_refresh_token()
                self.load_tasks()
                self.scan_phase()
                self.process_phase()
                self.update_phase()
                self.summary_report()

                time.sleep(60)

            except Exception as e:
                log_separator(self.log_file)
                log_to_file(self.log_file, f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Ü–∏–∫–ª–µ: {e}")
                time.sleep(10)

    def load_tasks(self):
        log_section("üß© üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–¥–∞—á –∏–∑ RotationsInfo", self.log_file)
        self.tasks = load_rotationsinfo_tasks(self.conn)

        if not self.tasks:
            log_section(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ RotationsInfo.")
            return

        log_section(self.log_file, f"üîÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.tasks)} –∑–∞–¥–∞—á.")
        for task in self.tasks:
            log_to_file(self.log_file, f"   ‚Ä¢ [Task] {task.source_table_type} | –°—Ç—Ä–∞–Ω–∏—Ü–∞: {task.source_page_name} | –î–∏–∞–ø–∞–∑–æ–Ω: {task.source_page_area}")
            task.assign_doc_ids(self.doc_id_map)

    # def check_and_refresh_token(self):
    #     log_section("üîê –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ Google API —Ç–æ–∫–µ–Ω–∞", self.log_file)

    #     token_path = "token.json"
    #     if not os.path.exists(token_path):
    #         log_to_file(self.log_file, f"‚ùå –§–∞–π–ª {token_path} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞.")
    #         raise FileNotFoundError("token.json –Ω–µ –Ω–∞–π–¥–µ–Ω")

    #     creds = Credentials.from_authorized_user_file(token_path)
    #     if creds.expired and creds.refresh_token:
    #         try:
    #             creds.refresh(Request())
    #             with open(token_path, 'w') as token_file:
    #                 token_file.write(creds.to_json())
    #             log_to_file(self.log_file, "üîÑ –¢–æ–∫–µ–Ω —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª—ë–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ token.json.")
    #         except Exception as e:
    #             log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞: {e}")
    #             raise
    #     else:
    #         log_to_file(self.log_file, "‚úÖ –¢–æ–∫–µ–Ω –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω.")

    def check_and_refresh_token(self):
        log_section("üîê –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ Google API —Ç–æ–∫–µ–Ω–∞", self.log_file)

        token_path = "token.json"
        creds = None

        if not os.path.exists(token_path):
            log_to_file(self.log_file, f"‚ùå –§–∞–π–ª {token_path} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –Ω–µ–≤–æ–∑–º–æ–∂–Ω–∞.")
            raise FileNotFoundError("token.json –Ω–µ –Ω–∞–π–¥–µ–Ω")

        creds = Credentials.from_authorized_user_file(token_path)
        if creds.expired and creds.refresh_token:
            try:
                creds.refresh(Request())
                with open(token_path, 'w') as token_file:
                    token_file.write(creds.to_json())
                log_to_file(self.log_file, "üîÑ –¢–æ–∫–µ–Ω —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª—ë–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ token.json.")
            except Exception as e:
                log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞: {e}")
                raise
        else:
            log_to_file(self.log_file, "‚úÖ –¢–æ–∫–µ–Ω –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω –∏ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è.")

#==================================================================================================
#  –§–∞–∑–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
#==================================================================================================

    def update_task_update_fields(self, task):
        """–û–±–Ω–æ–≤–∏—Ç—å last_update, update_quantity, update_failures –ø–æ—Å–ª–µ –≤—ã–≥—Ä—É–∑–∫–∏ –∑–∞–¥–∞—á–∏."""
        cursor = self.conn.cursor()

        cursor.execute(f"""
            UPDATE {task.source_table}
            SET
                last_update = ?,
                update_quantity = ?,
                update_failures = ?,
                need_update = ?
            WHERE id = ?
        """, (
            task.last_update.isoformat() if task.last_update else None,
            task.update_quantity,
            task.update_failures,
            task.need_update,
            task.id
        ))

        log_to_file(self.log_file, f"üíæ –°–æ—Ö—Ä–∞–Ω—è—é –≤ –ë–î [Task {task.name_of_process}] ‚Üí need_update={task.need_update}, hash={task.hash}")

        self.conn.commit()

    def check_sheet_exists(self, spreadsheet_id, sheet_name):
        try:
            metadata = self.service.spreadsheets().get(spreadsheetId=spreadsheet_id).execute()
            sheets = metadata.get('sheets', [])
            for sheet in sheets:
                title = sheet.get('properties', {}).get('title')
                if title == sheet_name:
                    return True
            return False
        except Exception as e:
            log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ª–∏—Å—Ç–∞ '{sheet_name}' –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ {spreadsheet_id}: {e}")
            return False

    def batch_get(self, service, spreadsheet_id, ranges, log_file, retries=5, delay_seconds=5):
        attempt = 0
        while attempt < retries:
            try:
                log_to_file(log_file, f"üì° batchGet (–ø–æ–ø—ã—Ç–∫–∞ {attempt+1}/{retries}) –¥–ª—è {spreadsheet_id}")
                result = service.spreadsheets().values().batchGet(
                    spreadsheetId=spreadsheet_id,
                    ranges=ranges,
                    majorDimension="ROWS"
                ).execute()
                value_ranges = result.get("valueRanges", [])
                data = {vr.get("range", ""): vr.get("values", []) for vr in value_ranges}
                log_to_file(log_file, f"‚úÖ batchGet —É—Å–ø–µ—à–µ–Ω. –ü–æ–ª—É—á–µ–Ω–æ –¥–∏–∞–ø–∞–∑–æ–Ω–æ–≤: {len(data)}")
                return data
            except HttpError as e:
                status = e.resp.status
                log_to_file(log_file, f"‚ùå HttpError {status}: {e}")
                if status in (429, 500, 503):
                    attempt += 1
                    log_to_file(log_file, f"‚è≥ –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay_seconds} —Å–µ–∫—É–Ω–¥...")
                    time.sleep(delay_seconds)
                elif status == 401:
                    log_to_file(log_file, "üîí –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (401). –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ.")
                    return {}
                else:
                    return {}
            except Exception as e:
                if any(x in str(e) for x in ["SSL", "handshake", "timed out"]):
                    attempt += 1
                    log_to_file(log_file, f"‚è≥ –°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ '{e}'. –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {delay_seconds} —Å–µ–∫—É–Ω–¥...")
                    time.sleep(delay_seconds)
                else:
                    log_to_file(log_file, f"‚ùå –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
                    return {}
        log_to_file(log_file, "‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ batchGet.")
        return {}
    
    def summary_report(self):
        log_section("üìà –ò—Ç–æ–≥–∏ —Ç–µ–∫—É—â–µ–≥–æ —Ü–∏–∫–ª–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", self.log_file)

        scan = getattr(self, "metrics_scan", {"ready": 0, "success": 0, "failed": 0})
        process = getattr(self, "metrics_process", {"success": 0, "skipped": 0, "failed": 0})
        update = getattr(self, "metrics_update", {"updated": 0, "skipped": 0, "failed": 0})

        log_to_file(self.log_file, "üîç –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ:")
        log_to_file(self.log_file, f"   ‚Ä¢ –ì–æ—Ç–æ–≤–æ –∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é: {scan['ready']}")
        log_to_file(self.log_file, f"   ‚Ä¢ –£—Å–ø–µ—à–Ω–æ –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ: {scan['success']}")
        log_to_file(self.log_file, f"   ‚Ä¢ –û—à–∏–±–æ–∫/–ø—Ä–æ–ø—É—Å–∫–æ–≤: {scan['failed']}")

        log_to_file(self.log_file, "üõ†Ô∏è –û–±—Ä–∞–±–æ—Ç–∫–∞:")
        log_to_file(self.log_file, f"   ‚Ä¢ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {process['success']}")
        log_to_file(self.log_file, f"   ‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–æ: {process['skipped']}")
        log_to_file(self.log_file, f"   ‚Ä¢ –û—à–∏–±–æ–∫: {process['failed']}")

        log_to_file(self.log_file, "üîº –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ:")
        log_to_file(self.log_file, f"   ‚Ä¢ –£—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–æ: {update['updated']}")
        log_to_file(self.log_file, f"   ‚Ä¢ –ü—Ä–æ–ø—É—â–µ–Ω–æ: {update['skipped']}")
        log_to_file(self.log_file, f"   ‚Ä¢ –û—à–∏–±–æ–∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {update['failed']}")

        log_to_file(self.log_file, "=" * 100)

    def scan_phase(self):
        log_section("üîç –§–∞–∑–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è", self.log_file)

        if not self.tasks:
            log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.")
            self.metrics_scan = {"ready": 0, "success": 0, "failed": 0}
            return

        ready_tasks = [task for task in self.tasks if task.is_ready_to_scan()]
        if not ready_tasks:
            log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á, –≥–æ—Ç–æ–≤—ã—Ö –∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é.")
            self.metrics_scan = {"ready": 0, "success": 0, "failed": 0}
            return

        log_to_file(self.log_file, f"üîé –ù–∞–π–¥–µ–Ω–æ {len(ready_tasks)} –∑–∞–¥–∞—á, –≥–æ—Ç–æ–≤—ã—Ö –∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é:")
        for task in ready_tasks:
            log_to_file(self.log_file, f"   ‚Ä¢ [Task] {task.name_of_process} | source_page_name: {task.source_page_name} | –î–∏–∞–ø–∞–∑–æ–Ω: {task.source_page_area}")

        # === –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ scan_group ===
        scan_groups = defaultdict(list)
        for task in ready_tasks:
            if not task.assign_doc_ids(self.doc_id_map):
                log_to_file(self.log_file, f"‚ö†Ô∏è [Task {task.name_of_process}] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–ø–æ—Å—Ç–∞–≤–∏—Ç—å doc_id. –ü—Ä–æ–ø—É—Å–∫.")
                continue
            scan_groups[task.scan_group].append(task)

        total_success = 0
        total_failed = 0

        for scan_group, group_tasks in scan_groups.items():
            log_separator(self.log_file)
            log_to_file(self.log_file, f"üìò –û–±—Ä–∞–±–æ—Ç–∫–∞ scan_group: {scan_group} ({len(group_tasks)} –∑–∞–¥–∞—á)")

            if not group_tasks:
                log_to_file(self.log_file, "‚ö™ –í –≥—Ä—É–ø–ø–µ –Ω–µ—Ç –∑–∞–¥–∞—á.")
                continue

            doc_id = group_tasks[0].source_doc_id
            unique_sheet_names = set(task.source_page_name for task in group_tasks)
            log_to_file(self.log_file, f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ª–∏—Å—Ç—ã (source_page_name): {unique_sheet_names}")

            # === –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –ª–∏—Å—Ç–æ–≤ ===
            exists_map = {}
            for sheet_name in unique_sheet_names:
                exists_map[sheet_name] = self.check_sheet_exists(doc_id, sheet_name)
                log_to_file(
                    self.log_file,
                    f"{'‚úÖ' if exists_map[sheet_name] else '‚ö†Ô∏è'} –õ–∏—Å—Ç '{sheet_name}' {'–Ω–∞–π–¥–µ–Ω' if exists_map[sheet_name] else '–Ω–µ –Ω–∞–π–¥–µ–Ω'}"
                )

            valid_tasks = []
            for task in group_tasks:
                sheet_name = task.actual_tab
                if exists_map.get(sheet_name):
                    log_to_file(self.log_file, f"‚û°Ô∏è –õ–∏—Å—Ç '{sheet_name}' –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –∑–∞–¥–∞—á–∏ {task.name_of_process}.")
                    valid_tasks.append(task)
                else:
                    log_to_file(self.log_file, f"‚õî –ü—Ä–æ–ø—É—Å–∫ –∑–∞–¥–∞—á–∏ {task.name_of_process}: –ª–∏—Å—Ç '{sheet_name}' –Ω–µ –Ω–∞–π–¥–µ–Ω.")
                    task.update_after_scan(success=False)
                    self.update_task_scan_fields(task)
                    total_failed += 1

            if not valid_tasks:
                log_to_file(self.log_file, f"‚ö™ –í—Å–µ –∑–∞–¥–∞—á–∏ –≥—Ä—É–ø–ø—ã {scan_group} –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω—ã. –ü—Ä–æ–ø—É—Å–∫ batchGet.")
                continue

            # === –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º ===
            range_to_tasks = defaultdict(list)
            for task in valid_tasks:
                range_str = f"{task.source_page_name}!{task.source_page_area}"
                range_to_tasks[range_str].append(task)

            ranges = list(range_to_tasks.keys())

            log_to_file(self.log_file, f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ batchGet –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–∞ {task.source_table_type}:")
            for r in ranges:
                log_to_file(self.log_file, f"   ‚Ä¢ {r}")

            # === –ó–∞–ø—Ä–æ—Å –¥–∞–Ω–Ω—ã—Ö ===
            response_data = self.batch_get(self.service, doc_id, ranges, self.log_file)

            if not response_data:
                log_to_file(self.log_file, "‚ùå –ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç batchGet. –û—Ç–º–µ—á–∞–µ–º –∑–∞–¥–∞—á–∏ –∫–∞–∫ –Ω–µ—É–¥–∞—á–Ω—ã–µ.")
                for task in valid_tasks:
                    task.update_after_scan(success=False)
                    self.update_task_scan_fields(task)
                    total_failed += 1
                continue

            # === –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ ===
            normalized_response = {}
            for k, v in response_data.items():
                clean_key = k.replace("'", "")
                if "!" in clean_key:
                    sheet_name, cells_range = clean_key.split("!", 1)
                    normalized_response[(sheet_name.strip(), cells_range.strip())] = v

            log_to_file(self.log_file, f"üì• –ü–æ–ª—É—á–µ–Ω—ã –¥–∏–∞–ø–∞–∑–æ–Ω—ã: {list(normalized_response.keys())}")

            for task in valid_tasks:
                expected_sheet = task.source_page_name.strip()
                expected_area_start = task.source_page_area.split(":")[0].strip()

                matched_values = None
                for (sheet_name, cells_range), values in normalized_response.items():
                    if sheet_name == expected_sheet and cells_range.startswith(expected_area_start):
                        matched_values = values
                        break

                if matched_values:
                    task.raw_values_json = matched_values
                    task.update_after_scan(success=True)
                    self.update_task_scan_fields(task)
                    log_to_file(self.log_file, f"‚úÖ [Task {task.name_of_process}] –ù–∞–π–¥–µ–Ω –¥–∏–∞–ø–∞–∑–æ–Ω {sheet_name}!{cells_range}, —Å—Ç—Ä–æ–∫: {len(matched_values)}")
                    for i, row in enumerate(matched_values[:5]):
                        log_to_file(self.log_file, f"      [{i+1}] {row}")
                    if len(matched_values) > 5:
                        log_to_file(self.log_file, f"      ...–µ—â—ë {len(matched_values) - 5} —Å—Ç—Ä–æ–∫ —Å–∫—Ä—ã—Ç–æ")
                    total_success += 1
                else:
                    task.update_after_scan(success=False)
                    self.update_task_scan_fields(task)
                    log_to_file(self.log_file, f"‚ö†Ô∏è [Task {task.name_of_process}] –î–∏–∞–ø–∞–∑–æ–Ω {expected_sheet}!{task.source_page_area} –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç.")
                    total_failed += 1

        log_separator(self.log_file)
        log_to_file(self.log_file, "üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ñ–∞–∑—ã —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è:")
        log_to_file(self.log_file, f"   ‚Ä¢ ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ—Ç—Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–æ: {total_success}")
        log_to_file(self.log_file, f"   ‚Ä¢ ‚ùå –û—à–∏–±–æ–∫/–ø—Ä–æ–ø—É—Å–∫–æ–≤: {total_failed}")
        log_to_file(self.log_file, f"   ‚Ä¢ üü° –í—Å–µ–≥–æ –≥–æ—Ç–æ–≤—ã—Ö –∫ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é: {len(ready_tasks)}")

        self.metrics_scan = {
            "ready": len(ready_tasks),
            "success": total_success,
            "failed": total_failed
        }

    #==================================================================================================
    #  –§–∞–∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    #==================================================================================================

    def update_task_process_fields(self, task):
        """–û–±–Ω–æ–≤–∏—Ç—å values_json, hash –∏ need_update –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á–∏."""
        cursor = self.conn.cursor()

        table_name = "SheetsInfo" if task.source_table_type == "SheetsInfo" else "RotationsInfo"

        cursor.execute(f"""
            UPDATE {table_name}
            SET
                hash = ?,
                values_json = ?,
                need_update = ?
            WHERE id = ?
        """, (
            task.hash,
            str(task.values_json) if task.values_json else None,
            task.need_update,
            task.id
        ))
        
        log_to_file(self.log_file, f"üíæ –°–æ—Ö—Ä–∞–Ω—è—é –≤ –ë–î [Task {task.name_of_process}] ‚Üí need_update={task.need_update}, hash={task.hash}")

        self.conn.commit()

    def process_phase(self):
        log_section("üõ†Ô∏è –§–∞–∑–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏", self.log_file)

        if not self.tasks:
            log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
            self.metrics_process = {"success": 0, "skipped": 0, "failed": 0}
            return

        processed = 0
        skipped = 0
        failed = 0

        for task in self.tasks:
            if not task.raw_values_json:
                log_to_file(self.log_file, f"‚ö™ [Task {task.name_of_process}] –ù–µ—Ç —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
                skipped += 1
                continue

            try:
                log_to_file(self.log_file, f"üîß –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–¥–∞—á–∏ [Task {task.name_of_process}]...")

                try:
                    task.process_raw_value()
                    log_to_file(self.log_file, f"üì¶ [Task {task.name_of_process}] –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {len(task.values_json)} —Å—Ç—Ä–æ–∫.")
                    for i, row in enumerate(task.values_json[:5]):
                        log_to_file(self.log_file, f"      [{i+1}] {row}")
                    if len(task.values_json) > 5:
                        log_to_file(self.log_file, f"      ...–µ—â—ë {len(task.values_json) - 5} —Å—Ç—Ä–æ–∫ —Å–∫—Ä—ã—Ç–æ")
                except Exception as e:
                    log_to_file(self.log_file, f"‚ùå [Task {task.name_of_process}] –û—à–∏–±–∫–∞ –≤ process_raw_value: {e}")
                    failed += 1
                    continue

                try:
                    old_hash = task.hash
                    task.check_for_update()
                    new_hash = task.hash

                    log_to_file(self.log_file, "")
                    log_to_file(self.log_file, f"üßÆ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ö–µ—à–∞ [Task {task.name_of_process}]:")
                    log_to_file(self.log_file, f"     ‚Ä¢ –°—Ç–∞—Ä—ã–π —Ö–µ—à: {old_hash}")
                    log_to_file(self.log_file, f"     ‚Ä¢ –ù–æ–≤—ã–π —Ö–µ—à : {new_hash}")
                    log_to_file(self.log_file, "")

                    if task.need_update:
                        log_to_file(self.log_file, f"üîÅ –ò–∑–º–µ–Ω–µ–Ω–∏—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã ‚Äî –∑–∞–¥–∞—á–∞ –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω–∞.")
                    else:
                        log_to_file(self.log_file, f"‚ö™ –ò–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ—Ç ‚Äî –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")
                except Exception as e:
                    log_to_file(self.log_file, f"‚ùå [Task {task.name_of_process}] –û—à–∏–±–∫–∞ –≤ check_for_update: {e}")
                    failed += 1
                    continue

                self.update_task_process_fields(task)
                log_to_file(self.log_file, f"‚úÖ [Task {task.name_of_process}] –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∏ –∑–∞–ø–∏—Å–∞–Ω–∞ –≤ –ë–î.")
                log_separator(self.log_file)
                processed += 1

            except Exception as e:
                log_to_file(self.log_file, f"‚ùå [Task {task.name_of_process}] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
                failed += 1

        log_to_file(self.log_file, "üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ñ–∞–∑—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
        log_to_file(self.log_file, f"   ‚Ä¢ ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {processed}")
        log_to_file(self.log_file, f"   ‚Ä¢ ‚ö™ –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö): {skipped}")
        log_to_file(self.log_file, f"   ‚Ä¢ ‚ùå –° –æ—à–∏–±–∫–∞–º–∏: {failed}")

        self.metrics_process = {
            "success": processed,
            "skipped": skipped,
            "failed": failed
        }


    #==================================================================================================
    #  –§–∞–∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
    #==================================================================================================

    def update_phase(self):
        log_section("üîº –§–∞–∑–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è", self.log_file)

        if not self.tasks:
            log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è.")
            self.metrics_update = {"updated": 0, "skipped": 0, "failed": 0}
            return

        tasks_to_update = [task for task in self.tasks if task.need_update == 1]

        if not tasks_to_update:
            log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤ Google Sheets.")
            self.metrics_update = {"updated": 0, "skipped": 0, "failed": 0}
            return

        log_to_file(self.log_file, f"üîÑ –ù–∞—á–∞–ª–æ —Ñ–∞–∑—ã –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è. –ó–∞–¥–∞—á –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏: {len(tasks_to_update)}.")
        log_to_file(self.log_file, "=" * 100)

        total_updated = 0
        total_failed = 0
        total_skipped = 0

        tasks_by_update_group = defaultdict(list)
        for task in tasks_to_update:
            tasks_by_update_group[task.update_group].append(task)

        for update_group, group_tasks in tasks_by_update_group.items():
            log_to_file(self.log_file, f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –≥—Ä—É–ø–ø—ã –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {update_group} ({len(group_tasks)} –∑–∞–¥–∞—á).")
            log_separator(self.log_file)

            tasks_by_doc = defaultdict(list)
            for task in group_tasks:
                tasks_by_doc[task.target_doc_id].append(task)

            for doc_id, tasks in tasks_by_doc.items():
                log_to_file(self.log_file, f"üìÑ –†–∞–±–æ—Ç–∞–µ–º —Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–º {task.source_table_type} ({len(tasks)} –∑–∞–¥–∞—á).")

                batch_data = []
                for task in tasks:
                    if not task.values_json:
                        log_to_file(self.log_file, f"‚ö™ [Task {task.name_of_process}] –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏, –ø—Ä–æ–ø—É—Å–∫.")
                        total_skipped += 1
                        continue

                    batch_data.append({
                        "range": f"{task.target_page_name}!{task.target_page_area}",
                        "values": task.values_json
                    })

                if not batch_data:
                    log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è batchUpdate.")
                    continue

                success, error = self.batch_update(self.service, doc_id, batch_data, self.log_file)

                if success:
                    for task in tasks:
                        task.update_after_upload(success=True)
                        self.update_task_update_fields(task)
                    log_to_file(self.log_file, f"‚úÖ –£—Å–ø–µ—à–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥—Ä—É–ø–ø—ã ({len(tasks)} –∑–∞–¥–∞—á) –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º.")
                    total_updated += len(tasks)
                else:
                    log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ batchUpdate: {error}")
                    log_to_file(self.log_file, "üîÑ –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –ø–æ—à—Ç—É—á–Ω—É—é –æ—Ç–ø—Ä–∞–≤–∫—É –∑–∞–¥–∞—á.")

                    for task in tasks:
                        if not task.values_json:
                            continue

                        single_data = [{
                            "range": f"{task.target_page_name}!{task.target_page_area}",
                            "values": task.values_json
                        }]

                        single_success, single_error = self.batch_update(self.service, doc_id, single_data, self.log_file)

                        if single_success:
                            task.update_after_upload(success=True)
                            log_to_file(self.log_file, f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∞ –∑–∞–¥–∞—á–∞ [Task {task.name_of_process}] –æ—Ç–¥–µ–ª—å–Ω–æ.")
                            total_updated += 1
                        else:
                            task.update_after_upload(success=False)
                            log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è [Task {task.name_of_process}] –æ—Ç–¥–µ–ª—å–Ω–æ: {single_error}")
                            total_failed += 1

                        self.update_task_update_fields(task)

                time.sleep(2)

        log_to_file(self.log_file, "üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ñ–∞–∑—ã –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è:")
        log_to_file(self.log_file, f"   ‚Ä¢ ‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–æ: {total_updated}")
        log_to_file(self.log_file, f"   ‚Ä¢ ‚ùå –ù–µ—É–¥–∞—á–Ω—ã—Ö –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π: {total_failed}")
        log_to_file(self.log_file, f"   ‚Ä¢ ‚ö™ –ü—Ä–æ–ø—É—â–µ–Ω–æ (–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö): {total_skipped}")
        log_to_file(self.log_file, f"   ‚Ä¢ üîÅ –í—Å–µ–≥–æ –∑–∞–¥–∞—á –≤ –æ—á–µ—Ä–µ–¥–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è: {len(tasks_to_update)}")

        self.metrics_update = {
            "updated": total_updated,
            "failed": total_failed,
            "skipped": total_skipped
        }


    def update_tasks_batch(self, spreadsheet_id, tasks):
        """–ü—ã—Ç–∞–µ—Ç—Å—è –æ–±–Ω–æ–≤–∏—Ç—å –∑–∞–¥–∞—á–∏ –æ–¥–Ω–∏–º batchUpdate, –∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ ‚Äî –ø–æ—à—Ç—É—á–Ω–æ."""
        batch_data = []
        for task in tasks:
            if not task.values_json:
                log_to_file(self.log_file, f"‚ö™ [Task {task.name_of_process}] –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏, –ø—Ä–æ–ø—É—Å–∫.")
                continue

            batch_data.append({
                "range": f"{task.target_page_name}!{task.target_page_area}",
                "values": task.values_json
            })

        if not batch_data:
            log_to_file(self.log_file, "‚ö™ –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è batchUpdate.")
            return

        success, error = self.batch_update(self.service, spreadsheet_id, batch_data, self.log_file)

        if success:
            for task in tasks:
                task.update_after_upload(success=True)
                self.update_task_update_fields(task)
            log_to_file(self.log_file, f"‚úÖ –£—Å–ø–µ—à–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≥—Ä—É–ø–ø—ã ({len(tasks)} –∑–∞–¥–∞—á) –æ–¥–Ω–∏–º –∑–∞–ø—Ä–æ—Å–æ–º.")
        else:
            log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ batchUpdate –¥–ª—è –≥—Ä—É–ø–ø—ã: {error}")
            log_to_file(self.log_file, "üîÑ –ü–µ—Ä–µ—Ö–æ–¥–∏–º –Ω–∞ –ø–æ—à—Ç—É—á–Ω—É—é –æ—Ç–ø—Ä–∞–≤–∫—É –∑–∞–¥–∞—á.")

            # –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –æ–¥–∏–Ω–æ—á–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–µ
            for task in tasks:
                if not task.values_json:
                    continue

                single_data = [{
                    "range": f"{task.target_page_name}!{task.target_page_area}",
                    "values": task.values_json
                }]

                single_success, single_error = self.batch_update(self.service, spreadsheet_id, single_data, self.log_file)

                if single_success:
                    task.update_after_upload(success=True)
                    log_to_file(self.log_file, f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∞ –∑–∞–¥–∞—á–∞ [Task {task.name_of_process}] –æ—Ç–¥–µ–ª—å–Ω–æ.")
                else:
                    task.update_after_upload(success=False)
                    log_to_file(self.log_file, f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è [Task {task.name_of_process}] –æ—Ç–¥–µ–ª—å–Ω–æ: {single_error}")

                self.update_task_update_fields(task)

    def batch_update(self, service, spreadsheet_id, batch_data, log_file, retries=3, delay_seconds=10):
        """ –û—Ç–ø—Ä–∞–≤–∫–∞ batchUpdate –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö """
        attempt = 0

        while attempt < retries:
            try:
                body = {
                    "valueInputOption": "USER_ENTERED",
                    "data": batch_data
                }
                service.spreadsheets().values().batchUpdate(
                    spreadsheetId=spreadsheet_id,
                    body=body
                ).execute()

                return True, None  # –£—Å–ø–µ—Ö

            except HttpError as e:
                status_code = e.resp.status
                log_to_file(log_file, f"‚ùå –û—à–∏–±–∫–∞ HTTP {status_code} –ø—Ä–∏ batchUpdate: {e}")

                if status_code in (429, 500, 503):  # –ü–µ—Ä–µ–≥—Ä—É–∑–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ –∏–ª–∏ –ª–∏–º–∏—Ç
                    attempt += 1
                    log_to_file(log_file, f"‚è≥ –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ {attempt}/{retries} —á–µ—Ä–µ–∑ {delay_seconds} —Å–µ–∫—É–Ω–¥...")
                    time.sleep(delay_seconds)
                elif status_code == 401:  # –ü—Ä–æ–±–ª–µ–º—ã —Å –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π
                    log_to_file(log_file, "üîí –û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ (401). –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±–Ω–æ–≤–∏—Ç—å —Ç–æ–∫–µ–Ω.")
                    return False, f"–û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏: {e}"
                else:
                    return False, str(e)

            except Exception as e:
                log_to_file(log_file, f"‚ùå –ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ batchUpdate: {e}")
                return False, str(e)

        return False, "–ü—Ä–µ–≤—ã—à–µ–Ω–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –æ—Ç–ø—Ä–∞–≤–∫–∏ batchUpdate."
