import time
from datetime import datetime
import traceback
import json

from config import WARSAW_TZ
from database import update_sheet_task_data, update_sheet_import_data, update_rotation_row_data, set_need_update_true_below
from logger import log_to_file
from utils import process_data_by_method


def handle_fetched_data(value_ranges, range_to_tasks, changed_update_groups, table_name, log_file):
    """
    –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç Google Sheets: –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö—ç—à–∞, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ, –∑–∞–ø–∏—Å—å –≤ –ë–î.
    """
    for sheet_range, value_range in zip(range_to_tasks.keys(), value_ranges):
        values = value_range.get("values", [])

        for task in range_to_tasks[sheet_range]:
            try:
                process_single_task(task, values, sheet_range, changed_update_groups, table_name, log_file)
            except Exception as e:
                log_to_file(log_file, f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ ID={task['id']}: {str(e)}\n{traceback.format_exc(limit=1)}")

def process_single_task(task, values, sheet_range, changed_update_groups, table_name, log_file):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É –∑–∞–¥–∞—á—É:
    - –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è
    - –í—ã–∑—ã–≤–∞–µ—Ç –Ω—É–∂–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    - –í—ã—á–∏—Å–ª—è–µ—Ç —Ö—ç—à
    - –ï—Å–ª–∏ —Ö—ç—à –∏–∑–º–µ–Ω–∏–ª—Å—è ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –≥—Ä—É–ø–ø—É –∫ –∏–º–ø–æ—Ä—Ç—É
    - –õ–æ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    """
    task_id = task["id"]
    log_to_file(log_file, f"üü¢ ID={task_id} | –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞: {sheet_range}")

    # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è
    now_str = datetime.now(WARSAW_TZ).isoformat()
    task["last_scan"] = now_str
    task["scan_quantity"] = task.get("scan_quantity", 0) + 1

    if not values:
        task["scan_failures"] = task.get("scan_failures", 0) + 1
        log_to_file(log_file, "  ‚ùå –ü—É—Å—Ç–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω.")
        update_sheet_task_data(task, table_name, log_file)
        return

    # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    method = task.get("process_data_method") or "process_default"
    values_json, new_hash = process_data_by_method(method, values, log_file)

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ö—ç—à–µ–π
    old_hash = task.get("hash")
    if new_hash == old_hash:
        log_to_file(log_file, "  ‚úÖ –î–∞–Ω–Ω—ã–µ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å (hash —Å–æ–≤–ø–∞–¥–∞–µ—Ç).")
    else:
        task["hash"] = new_hash
        task["values_json"] = values_json
        log_to_file(log_file, "  üîÅ –î–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω—ã (hash –æ–±–Ω–æ–≤–ª—ë–Ω).")

        update_group = task.get("update_group")
        if update_group and update_group not in changed_update_groups:
            changed_update_groups.append(update_group)

    # –§–∏–Ω–∞–ª—å–Ω—ã–π –ª–æ–≥ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
    log_task_summary(task, log_file)
    update_sheet_task_data(task, table_name, log_file)


def log_task_summary(task, log_file):
    """
    –õ–æ–≥–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–¥–∞—á–µ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏.
    """
    log_to_file(log_file, f"üì¶ ID={task['id']} | –ò—Ç–æ–≥:")
    for key in ["last_scan", "scan_quantity", "scan_failures", "hash"]:
        value = task.get(key)
        log_to_file(log_file, f"   {key} = {value}")


def perform_group_import(sheet, group_tasks, table_name, log_file):
    """
    –ü–∞–∫–µ—Ç–Ω—ã–π –∏–º–ø–æ—Ä—Ç –∑–∞–¥–∞—á –≥—Ä—É–ø–ø—ã. –ü–æ–≤—Ç–æ—Ä—è–µ—Ç –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö, –∏—Å–∫–ª—é—á–∞—è –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∑–∞–¥–∞—á–∏.
    """
    max_time = 60  # —Å–µ–∫—É–Ω–¥
    retry_delay = 1
    start_time = time.time()

    tasks = list(group_tasks)
    update_group = tasks[0].get("update_group", "–±–µ–∑ –∏–º–µ–Ω–∏")
    log_to_file(log_file, "=" * 30 + "\n")

    log_to_file(log_file, f"üü° –ó–∞–ø—É—Å–∫ –∏–º–ø–æ—Ä—Ç–∞ –≥—Ä—É–ø–ø—ã: {update_group} ({len(tasks)} –∑–∞–¥–∞—á)")

    while tasks and (time.time() - start_time < max_time):

        data = prepare_batch_data(tasks, table_name, log_file)

        if not data:
            log_to_file(log_file, "‚ö†Ô∏è –í—Å–µ –∑–∞–¥–∞—á–∏ –∏—Å–∫–ª—é—á–µ–Ω—ã ‚Äî –Ω–µ—á–µ–≥–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å.")
            break

        log_task_details(tasks, log_file)

        try:
            log_to_file(log_file, f"üß™ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ:\n{json.dumps(data, ensure_ascii=False)[:500]}")

            spreadsheet_id = tasks[0]["target_doc_id"]
            sheet.values().batchUpdate(
                spreadsheetId=spreadsheet_id,
                body={"valueInputOption": "RAW", "data": data}
            ).execute()

            now_str = datetime.now(WARSAW_TZ).isoformat()
            for task in tasks:
                task["last_update"] = now_str
                task["update_quantity"] = task.get("update_quantity", 0) + 1
                update_sheet_import_data(task, table_name, log_file)
                log_to_file(log_file, f"‚úÖ ID={task['id']} | –ò–º–ø–æ—Ä—Ç —É—Å–ø–µ—à–µ–Ω")
                log_to_file(log_file, "===========================")

            break  # –∏–º–ø–æ—Ä—Ç –ø—Ä–æ—à–µ–ª ‚Äî –≤—ã—Ö–æ–¥–∏–º

        except Exception as err:
            error_text = str(err)
            log_to_file(log_file, f"‚ùå –û—à–∏–±–∫–∞ batchUpdate:\n{error_text}\n{traceback.format_exc(limit=1)}")

            failed_task = find_failed_task(tasks, error_text, log_file)
            if failed_task:
                failed_task["update_failures"] = failed_task.get("update_failures", 0) + 1
                update_sheet_import_data(failed_task, table_name, log_file)
                log_to_file(log_file, f"‚ö†Ô∏è ID={failed_task['id']} | –ò—Å–∫–ª—é—á—ë–Ω –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏")
                tasks.remove(failed_task)
                log_to_file(log_file, "===========================")

            time.sleep(retry_delay)


def prepare_batch_data(tasks, table_name, log_file):
    prepared = []
    to_exclude = []

    for task in tasks:
        try:
            raw = task.get("values_json") or "[]"
            values = json.loads(raw)

            # –ó–∞—â–∏—Ç–∞: –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ø–∏—Å–∫–∏ —Å–ø–∏—Å–∫–æ–≤
            if not isinstance(values, list) or any(not isinstance(row, list) for row in values):
                raise ValueError("–î–∞–Ω–Ω—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Å–ø–∏—Å–∫–∞–º–∏ —Å—Ç—Ä–æ–∫ (2D-–º–∞—Å—Å–∏–≤)")

            prepared.append({
                "range": f"'{task['target_page_name']}'!{task['target_page_area']}",
                "values": values
            })

        except Exception as e:
            task["update_failures"] = task.get("update_failures", 0) + 1
            update_sheet_import_data(task, table_name, log_file)
            log_to_file(log_file, f"‚ö†Ô∏è –û—à–∏–±–∫–∞ JSON –∏–ª–∏ –¥–∞–Ω–Ω—ã—Ö –≤ ID={task['id']}: {str(e)}")
            to_exclude.append(task)

    for t in to_exclude:
        tasks.remove(t)

    return prepared



def find_failed_task(tasks, error_text, log_file):
    """
    –ù–∞—Ö–æ–¥–∏—Ç –∑–∞–¥–∞—á—É, –ø–æ –∫–æ—Ç–æ—Ä–æ–π –º–æ–≥–ª–∞ –ø—Ä–æ–∏–∑–æ–π—Ç–∏ –æ—à–∏–±–∫–∞ batchUpdate.
    –ò–Ω–∞—á–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—É—é.
    """
    for task in tasks:
        if task["target_page_name"] in error_text:
            return task
    return tasks[0] if tasks else None


def log_task_details(tasks, log_file):
    """
    –õ–æ–≥–∏—Ä—É–µ—Ç –∫–æ—Ä–æ—Ç–∫—É—é —Å–≤–æ–¥–∫—É –ø–æ –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–µ –ø–µ—Ä–µ–¥ –∏–º–ø–æ—Ä—Ç–æ–º
    """
    for task in tasks:
        log_to_file(log_file, f"üîÅ ID={task['id']}")
        log_to_file(log_file, f"   ‚û§ name_of_process: {task['name_of_process']}")
        log_to_file(log_file, f"   ‚û§ –ó–æ–Ω–∞ –æ—Ç–∫—É–¥–∞:  {task['source_table_type']}!{task['source_page_name']}!{task['source_page_area']}")
        log_to_file(log_file, f"   ‚û§ –ú–µ—Ç–æ–¥: {task['process_data_method']}")
        log_to_file(log_file, f"   ‚û§ –ó–æ–Ω–∞ –∫—É–¥–∞:  {task['target_table_type']}!{task['target_page_name']}!{task['target_page_area']}")
        try:
            data = json.loads(task["values_json"])
            log_to_file(log_file, f"   ‚û§ –†–∞–∑–º–µ—Ä: {len(data)}x{len(data[0]) if data else 0}")
        except Exception:
            log_to_file(log_file, "   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö JSON")
        log_to_file(log_file, "-" * 30)
    log_to_file(log_file, "=" * 30)
    return True

def handle_main_rotations_group(sheet, tasks, table_name, log_file):
    """
    –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç update_group='update_main': 8 —Ä–æ—Ç–∞—Ü–∏–π, –æ–¥–Ω–∞ —Ü–µ–ª—å, —Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä—è–¥–æ–∫.
    """

    ROTATION_ORDER = [
        "SHUFFLE Main",
        "VIP Main",
        "TURKISH Main",
        "GENERIC Main",
        "GSBJ Main",
        "LEGENDZ Main",
        "TRI-STAR Main",
        "TritonRL Main",
    ]

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∑–∞–¥–∞—á–∏ —Å—Ç—Ä–æ–≥–æ –ø–æ –ø–æ—Ä—è–¥–∫—É –Ω–∞–∑–≤–∞–Ω–∏–π
    tasks_by_name = {t["name_of_process"]: t for t in tasks}
    sorted_tasks = [tasks_by_name[name] for name in ROTATION_ORDER if name in tasks_by_name]

    prev_end = 1
    for idx, task in enumerate(sorted_tasks):
        task_id = task["id"]
        log_to_file(log_file, f"\nüåÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–æ—Ç–∞—Ü–∏–∏: {task['name_of_process']} (ID={task_id})")

        # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ JSON
        try:
            values = json.loads(task["values_json"])
        except Exception:
            log_to_file(log_file, "‚ö†Ô∏è –ü–æ–≤—Ä–µ–∂–¥—ë–Ω JSON. –ü—Ä–æ–ø—É—Å–∫.")
            continue

        if not values:
            log_to_file(log_file, "‚ö†Ô∏è –ü—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è. –ü—Ä–æ–ø—É—Å–∫.")
            continue

        row_count = len(values)
        old_start = task.get("start_row")
        old_end = task.get("end_row")
        old_hash = task.get("hash")
        need_update = task.get("need_update")

        # –í—ã—á–∏—Å–ª—è–µ–º –Ω–æ–≤—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
        new_start = prev_end
        new_end = new_start + row_count - 1

        # –ü–µ—Ä–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ (–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤–æ–æ–±—â–µ)
        if not old_start or not old_end or need_update is None:
            log_to_file(log_file, "üÜï –ü–µ—Ä–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–æ—Ç–∞—Ü–∏–∏")
            insert_rotation(sheet, task, new_start, new_end, values, table_name, log_file)
            update_rotation_row_data(task_id, new_start, new_end, task["hash"], values, False, table_name, log_file)
            set_need_update_true_below(task_id, table_name, log_file)

        # –ï—Å–ª–∏ need_update=True ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º –≤ –Ω–æ–≤—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω
        elif need_update:
            insert_rotation(sheet, task, new_start, new_end, values, table_name, log_file)
            clear_range(sheet, task["target_doc_id"], task["target_page_name"], new_end + 1, new_end + 1, table_name, log_file)
            update_rotation_row_data(task_id, new_start, new_end, task["hash"], values, False, table_name, log_file)
            set_need_update_true_below(task_id, table_name, log_file)

        # –ï—Å–ª–∏ —Ö—ç—à –∏ –¥–ª–∏–Ω–∞ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å ‚Äî –æ–±–Ω–æ–≤–ª—è–µ–º –∏ –∞–∫—Ç–∏–≤–∏—Ä—É–µ–º –Ω–∏–∂–Ω–∏–µ
        elif task["hash"] != old_hash and (old_end - old_start + 1 != row_count):
            insert_rotation(sheet, task, new_start, new_end, values, table_name, log_file)
            if old_end > new_end:
                clear_range(sheet, task["target_doc_id"], task["target_page_name"], new_end + 1, old_end, table_name, log_file)
            else:
                clear_range(sheet, task["target_doc_id"], task["target_page_name"], new_end + 1, new_end + 1, table_name, log_file)
            update_rotation_row_data(task_id, new_start, new_end, task["hash"], values, True, table_name, log_file)
            set_need_update_true_below(task_id, table_name, log_file)

        # –ï—Å–ª–∏ —Ö—ç—à –∏–∑–º–µ–Ω–∏–ª—Å—è, –∞ –¥–ª–∏–Ω–∞ —Å–æ–≤–ø–∞–¥–∞–µ—Ç ‚Äî –ø—Ä–æ—Å—Ç–æ –æ–±–Ω–æ–≤–ª—è–µ–º –Ω–∞ –º–µ—Å—Ç–µ
        elif task["hash"] != old_hash:
            insert_rotation(sheet, task, old_start, old_end, values, table_name, log_file)
            update_rotation_row_data(task_id, old_start, old_end, task["hash"], values, False, table_name, log_file)

        else:
            log_to_file(log_file, "‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫: –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ—Ç.")

        # –û–±–Ω–æ–≤–ª—è–µ–º prev_end –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π —Ä–æ—Ç–∞—Ü–∏–∏
        prev_end = new_end + 2


def insert_rotation(sheet, task, start_row, end_row, values, table_name, log_file):
    """
    –í—Å—Ç–∞–≤–ª—è–µ—Ç values –≤ –∑–æ–Ω—É 'target_page_name'!D{start_row}:AC{end_row}
    """
    target_range = f"'{task['target_page_name']}'!D{start_row}:AC{end_row}"
    log_to_file(log_file, f"‚¨áÔ∏è –í—Å—Ç–∞–≤–∫–∞ —Ä–æ—Ç–∞—Ü–∏–∏: {target_range} (—Å—Ç—Ä–æ–∫: {len(values)})")

    sheet.values().batchUpdate(
        spreadsheetId=task["target_doc_id"],
        body={
            "valueInputOption": "RAW",
            "data": [{
                "range": target_range,
                "values": values
            }]
        }
    ).execute()

def clear_range(sheet, spreadsheet_id, sheet_name, start_row, end_row, table_name, log_file):
    """
    –û—á–∏—â–∞–µ—Ç –¥–∏–∞–ø–∞–∑–æ–Ω D{start_row}:AC{end_row} –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º –ª–∏—Å—Ç–µ
    """
    if start_row > end_row:
        return  # –ù–µ—á–µ–≥–æ —á–∏—Å—Ç–∏—Ç—å

    clear_range = f"'{sheet_name}'!D{start_row}:AC{end_row}"
    log_to_file(log_file, f"üßπ –û—á–∏—Å—Ç–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞: {clear_range}")

    sheet.values().batchClear(
        spreadsheetId=spreadsheet_id,
        body={"ranges": [clear_range]}
    ).execute()
