# import time
# from datetime import datetime
# import traceback
# import json

# from config import WARSAW_TZ, SHEETS_LOG_FILE
# from database import update_sheet_task_data, update_sheet_import_data
# from logger import log_to_file
# from utils import process_data_by_method


# def handle_fetched_data(value_ranges, range_to_tasks, changed_update_groups, table_name, log_file):
#     """
#     –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Ç Google Sheets: –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ —Ö—ç—à–∞, —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ, –∑–∞–ø–∏—Å—å –≤ –ë–î.
#     """
#     for sheet_range, value_range in zip(range_to_tasks.keys(), value_ranges):
#         values = value_range.get("values", [])

#         for task in range_to_tasks[sheet_range]:
#             try:
#                 process_single_task(task, values, sheet_range, changed_update_groups, table_name, log_file)
#             except Exception as e:
#                 log_to_file(SHEETS_LOG_FILE, f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –∑–∞–¥–∞—á–µ ID={task['id']}: {str(e)}\n{traceback.format_exc(limit=1)}")

# def process_single_task(task, values, sheet_range, changed_update_groups):
#     """
#     –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–Ω—É –∑–∞–¥–∞—á—É:
#     - –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è
#     - –í—ã–∑—ã–≤–∞–µ—Ç –Ω—É–∂–Ω—ã–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏
#     - –í—ã—á–∏—Å–ª—è–µ—Ç —Ö—ç—à
#     - –ï—Å–ª–∏ —Ö—ç—à –∏–∑–º–µ–Ω–∏–ª—Å—è ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏—è –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –≥—Ä—É–ø–ø—É –∫ –∏–º–ø–æ—Ä—Ç—É
#     - –õ–æ–≥–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç
#     """
#     task_id = task["id"]
#     log_to_file(SHEETS_LOG_FILE, f"üü¢ ID={task_id} | –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ø–∞–∑–æ–Ω–∞: {sheet_range}")

#     # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è
#     now_str = datetime.now(WARSAW_TZ).isoformat()
#     task["last_scan"] = now_str
#     task["scan_quantity"] = task.get("scan_quantity", 0) + 1

#     if not values:
#         task["scan_failures"] = task.get("scan_failures", 0) + 1
#         log_to_file(SHEETS_LOG_FILE, "  ‚ùå –ü—É—Å—Ç–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω.")
#         update_sheet_task_data(task)
#         return

#     # –ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç–æ–¥–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏
#     method = task.get("process_data_method") or "process_default"
#     values_json, new_hash = process_data_by_method(method, values)

#     # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ö—ç—à–µ–π
#     old_hash = task.get("hash")
#     if new_hash == old_hash:
#         log_to_file(SHEETS_LOG_FILE, "  ‚úÖ –î–∞–Ω–Ω—ã–µ –Ω–µ –∏–∑–º–µ–Ω–∏–ª–∏—Å—å (hash —Å–æ–≤–ø–∞–¥–∞–µ—Ç).")
#     else:
#         task["hash"] = new_hash
#         task["values_json"] = values_json
#         log_to_file(SHEETS_LOG_FILE, "  üîÅ –î–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω—ã (hash –æ–±–Ω–æ–≤–ª—ë–Ω).")

#         update_group = task.get("update_group")
#         if update_group and update_group not in changed_update_groups:
#             changed_update_groups.append(update_group)

#     # –§–∏–Ω–∞–ª—å–Ω—ã–π –ª–æ–≥ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
#     log_task_summary(task)
#     update_sheet_task_data(task)


# def log_task_summary(task):
#     """
#     –õ–æ–≥–∏—Ä—É–µ—Ç –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–¥–∞—á–µ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏.
#     """
#     log_to_file(SHEETS_LOG_FILE, f"üì¶ ID={task['id']} | –ò—Ç–æ–≥:")
#     for key in ["last_scan", "scan_quantity", "scan_failures", "hash"]:
#         value = task.get(key)
#         log_to_file(SHEETS_LOG_FILE, f"   {key} = {value}")


# def perform_group_import(sheet, group_tasks):
#     """
#     –ü–∞–∫–µ—Ç–Ω—ã–π –∏–º–ø–æ—Ä—Ç –∑–∞–¥–∞—á –≥—Ä—É–ø–ø—ã. –ü–æ–≤—Ç–æ—Ä—è–µ—Ç –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö, –∏—Å–∫–ª—é—á–∞—è –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∑–∞–¥–∞—á–∏.
#     """
#     max_time = 60  # —Å–µ–∫—É–Ω–¥
#     retry_delay = 1
#     start_time = time.time()

#     tasks = list(group_tasks)
#     update_group = tasks[0].get("update_group", "–±–µ–∑ –∏–º–µ–Ω–∏")
#     log_to_file(SHEETS_LOG_FILE, "===========================")

#     log_to_file(SHEETS_LOG_FILE, f"üü° –ó–∞–ø—É—Å–∫ –∏–º–ø–æ—Ä—Ç–∞ –≥—Ä—É–ø–ø—ã: {update_group} ({len(tasks)} –∑–∞–¥–∞—á)")

#     while tasks and (time.time() - start_time < max_time):

#         data = prepare_batch_data(tasks)

#         if not data:
#             log_to_file(SHEETS_LOG_FILE, "‚ö†Ô∏è –í—Å–µ –∑–∞–¥–∞—á–∏ –∏—Å–∫–ª—é—á–µ–Ω—ã ‚Äî –Ω–µ—á–µ–≥–æ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å.")
#             break

#         log_task_details(tasks)

#         try:
#             log_to_file(SHEETS_LOG_FILE, f"üß™ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ:\n{json.dumps(data, ensure_ascii=False)[:500]}")

#             spreadsheet_id = tasks[0]["target_doc_id"]
#             sheet.values().batchUpdate(
#                 spreadsheetId=spreadsheet_id,
#                 body={"valueInputOption": "RAW", "data": data}
#             ).execute()

#             now_str = datetime.now(WARSAW_TZ).isoformat()
#             for task in tasks:
#                 task["last_update"] = now_str
#                 task["update_quantity"] = task.get("update_quantity", 0) + 1
#                 update_sheet_import_data(task)
#                 log_to_file(SHEETS_LOG_FILE, f"‚úÖ ID={task['id']} | –ò–º–ø–æ—Ä—Ç —É—Å–ø–µ—à–µ–Ω")
#                 log_to_file(SHEETS_LOG_FILE, "===========================")

#             break  # –∏–º–ø–æ—Ä—Ç –ø—Ä–æ—à–µ–ª ‚Äî –≤—ã—Ö–æ–¥–∏–º

#         except Exception as err:
#             error_text = str(err)
#             log_to_file(SHEETS_LOG_FILE, f"‚ùå –û—à–∏–±–∫–∞ batchUpdate:\n{error_text}\n{traceback.format_exc(limit=1)}")

#             failed_task = find_failed_task(tasks, error_text)
#             if failed_task:
#                 failed_task["update_failures"] = failed_task.get("update_failures", 0) + 1
#                 update_sheet_import_data(failed_task)
#                 log_to_file(SHEETS_LOG_FILE, f"‚ö†Ô∏è ID={failed_task['id']} | –ò—Å–∫–ª—é—á—ë–Ω –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏")
#                 tasks.remove(failed_task)
#                 log_to_file(SHEETS_LOG_FILE, "===========================")

#             time.sleep(retry_delay)


# def prepare_batch_data(tasks):
#     prepared = []
#     to_exclude = []

#     for task in tasks:
#         try:
#             raw = task.get("values_json") or "[]"
#             values = json.loads(raw)

#             # –ó–∞—â–∏—Ç–∞: –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ø–∏—Å–∫–∏ —Å–ø–∏—Å–∫–æ–≤
#             if not isinstance(values, list) or any(not isinstance(row, list) for row in values):
#                 raise ValueError("–î–∞–Ω–Ω—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Å–ø–∏—Å–∫–∞–º–∏ —Å—Ç—Ä–æ–∫ (2D-–º–∞—Å—Å–∏–≤)")

#             prepared.append({
#                 "range": f"'{task['target_page_name']}'!{task['target_page_area']}",
#                 "values": values
#             })

#         except Exception as e:
#             task["update_failures"] = task.get("update_failures", 0) + 1
#             update_sheet_import_data(task)
#             log_to_file(SHEETS_LOG_FILE, f"‚ö†Ô∏è –û—à–∏–±–∫–∞ JSON –∏–ª–∏ –¥–∞–Ω–Ω—ã—Ö –≤ ID={task['id']}: {str(e)}")
#             to_exclude.append(task)

#     for t in to_exclude:
#         tasks.remove(t)

#     return prepared



# def find_failed_task(tasks, error_text):
#     """
#     –ù–∞—Ö–æ–¥–∏—Ç –∑–∞–¥–∞—á—É, –ø–æ –∫–æ—Ç–æ—Ä–æ–π –º–æ–≥–ª–∞ –ø—Ä–æ–∏–∑–æ–π—Ç–∏ –æ—à–∏–±–∫–∞ batchUpdate.
#     –ò–Ω–∞—á–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—É—é.
#     """
#     for task in tasks:
#         if task["target_page_name"] in error_text:
#             return task
#     return tasks[0] if tasks else None


# def log_task_details(tasks):
#     """
#     –õ–æ–≥–∏—Ä—É–µ—Ç –∫–æ—Ä–æ—Ç–∫—É—é —Å–≤–æ–¥–∫—É –ø–æ –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–µ –ø–µ—Ä–µ–¥ –∏–º–ø–æ—Ä—Ç–æ–º
#     """
#     for task in tasks:
#         log_to_file(SHEETS_LOG_FILE, f"üîÅ ID={task['id']}")
#         log_to_file(SHEETS_LOG_FILE, f"   ‚û§ name_of_process: {task['name_of_process']}")
#         log_to_file(SHEETS_LOG_FILE, f"   ‚û§ –ó–æ–Ω–∞ –æ—Ç–∫—É–¥–∞:  {task['source_table_type']}!{task['source_page_name']}!{task['source_page_area']}")
#         log_to_file(SHEETS_LOG_FILE, f"   ‚û§ –ú–µ—Ç–æ–¥: {task['process_data_method']}")
#         log_to_file(SHEETS_LOG_FILE, f"   ‚û§ –ó–æ–Ω–∞ –∫—É–¥–∞:  {task['target_table_type']}!{task['target_page_name']}!{task['target_page_area']}")
#         try:
#             data = json.loads(task["values_json"])
#             log_to_file(SHEETS_LOG_FILE, f"   ‚û§ –†–∞–∑–º–µ—Ä: {len(data)}x{len(data[0]) if data else 0}")
#         except Exception:
#             log_to_file(SHEETS_LOG_FILE, "   ‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–∞–Ω–Ω—ã—Ö JSON")
#     return True